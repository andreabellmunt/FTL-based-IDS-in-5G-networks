{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FTL: Focusing on local nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Flatten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted preprocessing for https://github.com/polvalls9/Transfer-Learning-Based-Intrusion-Detection-in-5G-and-IoT-Networks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "\n",
    "    # Select the 'proto' and 'state' values that I want\n",
    "    data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "    data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] == 'PAR'), :]\n",
    "\n",
    "    # Extracting labels \n",
    "    data_labels = data[['label']]\n",
    "\n",
    "    # Drop the invalid features and select interested data features\n",
    "    data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "    \"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "\n",
    "    # Preprocess IP and ports features\n",
    "    # IP Source Address\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "\n",
    "    # IP Destination Address\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "    # Ports\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "    # Convert all ports with 0 decimal, and HEX to DEC\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    # Convert field to int format\n",
    "    data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "    data_features['sport'] = data_features['sport'].astype(int)\n",
    "    data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "    data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "    # Convert some fields to logarithmic\n",
    "    log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "    for col in log1p_col:\n",
    "        data_features[col] = data_features[col].apply(np.log1p)\n",
    "\n",
    "    # Create a complementary field of attack & Transform to One hot encoding - LABELS\n",
    "    normal=data_labels['label']\n",
    "    normal=normal.replace(1,2)\n",
    "    normal=normal.replace(0,1)\n",
    "    normal=normal.replace(2,0)\n",
    "\n",
    "    # Insert the new column in data labels\n",
    "    data_labels.insert(1, 'normal', normal)\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    # Transform to One hot encoding - FEATURES\n",
    "    data_features=pd.get_dummies(data_features)\n",
    "\n",
    "    # Value given for the missing columns\n",
    "    auxCol=0\n",
    "\n",
    "    # As we are using different datasets that might not have all representations, we are going to detect and add the missing columns \n",
    "    # The columns that can have types are: proto and state: need to check if all representations are done \n",
    "    state_cols = [col for col in data_features if col.startswith('state_')]\n",
    "    proto_cols = [col for col in data_features if col.startswith('proto_')]\n",
    "    \n",
    "    # Check if all columns are present\n",
    "    if 'state_PAR' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_PAR', auxCol, True)\n",
    "    if 'state_ACC' not in state_cols: \n",
    "        data_features.insert(data_features.shape[1], 'state_ACC', auxCol, True)\n",
    "    if 'state_ECO' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_ECO', auxCol, True)\n",
    "    if 'state_CON' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_CON', auxCol, True)\n",
    "    if 'state_FIN' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_FIN', auxCol, True)\n",
    "    if 'state_INT' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_INT', auxCol, True)\n",
    "    if 'state_REQ' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_REQ', auxCol, True)\n",
    "    if 'state_RST' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_RST', auxCol, True)\n",
    "    if 'proto_igmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_igmp', auxCol, True)\n",
    "    if 'proto_arp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_arp', auxCol, True)\n",
    "    if 'proto_icmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_icmp', auxCol, True)\n",
    "    if 'proto_udp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_udp', auxCol, True)\n",
    "    if 'proto_tcp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_tcp', auxCol, True)\n",
    "\n",
    "    # Normalize all data features\n",
    "    data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "    #Add dimension to data features\n",
    "    data_features = np.expand_dims(data_features, axis=2)\n",
    "    data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "    x = data_features\n",
    "    y = data_labels.to_numpy()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = models.load_model('./w_bot_5_5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.pop()\n",
    "bot.pop()\n",
    "bot.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../imb3_part1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(node, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = preprocessing(train)\n",
    "xval, yval = preprocessing(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(bot)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(units=192, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "filepath = '../node0w_ftl_imb3.hdf5'\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 2 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best\n",
    "optimizer = keras.optimizers.Adam(learning_rate=9e-4)\n",
    "model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "history = model.fit(xtrain, ytrain, epochs=20, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "model.save(filepath)\n",
    "\n",
    "print(model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../imb3_part2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(node, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = preprocessing(train)\n",
    "xval, yval = preprocessing(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(bot)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=448, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=224, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(rate=0.0))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "filepath = '../node1w_ftl_imb3.hdf5'\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 2 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-3)\n",
    "model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "history = model.fit(xtrain, ytrain, epochs=20, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "model.save(filepath)\n",
    "\n",
    "print(model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../imb3_part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(node, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = preprocessing(train)\n",
    "xval, yval = preprocessing(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(bot)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=224, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "filepath = '../node2w_ftl_imb3.hdf5'\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 2 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "history = model.fit(xtrain, ytrain, epochs=20, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "model.save(filepath)\n",
    "\n",
    "print(model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../imb3_part4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(node, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = preprocessing(train)\n",
    "xval, yval = preprocessing(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(bot)\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(Dense(units=448, activation='relu'))\n",
    "#model.add(Dropout(rate=0.3))\n",
    "#model.add(Dense(units=224, activation='relu'))\n",
    "#model.add(Dropout(rate=0.3))\n",
    "#model.add(Dense(units=64, activation='relu'))\n",
    "#model.add(Dropout(rate=0.0))\n",
    "#model.add(Dense(units=256, activation='relu'))\n",
    "#model.add(Dropout(rate=0.3))\n",
    "\n",
    "model.add(Dense(units=480, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(units=160, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=480, activation='relu'))\n",
    "model.add(Dropout(rate=0.0))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "filepath = '/node3w_ftl_imb3.hdf5'\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 2 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-3)\n",
    "model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "history = model.fit(xtrain, ytrain, epochs=20, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "model.save(filepath)\n",
    "\n",
    "print(model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pd.read_csv('../imb3_part5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(node, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain = preprocessing(train)\n",
    "xval, yval = preprocessing(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(bot)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=192, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(units=384, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(units=480, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "filepath = '../node4w_ftl_imb3.hdf5'\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 2 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "history = model.fit(xtrain, ytrain, epochs=20, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "model.save(filepath)\n",
    "\n",
    "print(model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
