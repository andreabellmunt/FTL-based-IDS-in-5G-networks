{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Federated Learning for attack detection: 5 nodes sharing weights** \n",
    "Includes the cases of the following aggregation functions: \n",
    "* Average\n",
    "* Median\n",
    "* Krum\n",
    "* Geometric Median\n",
    "\n",
    "Saving local and global models. Trained with UNSW-NB15 dataset partitions or Bot-IoT dataset partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static elements for all experiments (execute first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing extracted from https://github.com/polvalls9/Transfer-Learning-Based-Intrusion-Detection-in-5G-and-IoT-Networks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Disable warns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "\n",
    "    # Select the 'proto' and 'state' values that I want\n",
    "    data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "    data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] == 'PAR'), :]\n",
    "\n",
    "    # Extracting labels \n",
    "    data_labels = data[['label']]\n",
    "\n",
    "    # Drop the invalid features and select interested data features\n",
    "    data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "    \"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "\n",
    "    # Preprocess IP and ports features\n",
    "    # IP Source Address\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "\n",
    "    # IP Destination Address\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "    # Ports\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "    # Convert all ports with 0 decimal, and HEX to DEC\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    # Convert field to int format\n",
    "    data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "    data_features['sport'] = data_features['sport'].astype(int)\n",
    "    data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "    data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "    # Convert some fields to logarithmic\n",
    "    log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "    for col in log1p_col:\n",
    "        data_features[col] = data_features[col].apply(np.log1p)\n",
    "\n",
    "    # Create a complementary field of attack & Transform to One hot encoding - LABELS\n",
    "    normal=data_labels['label']\n",
    "    normal=normal.replace(1,2)\n",
    "    normal=normal.replace(0,1)\n",
    "    normal=normal.replace(2,0)\n",
    "\n",
    "    # Insert the new column in data labels\n",
    "    data_labels.insert(1, 'normal', normal)\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    # Transform to One hot encoding - FEATURES\n",
    "    data_features=pd.get_dummies(data_features)\n",
    "\n",
    "    # Value given for the missing columns\n",
    "    auxCol=0\n",
    "\n",
    "    # As we are using different datasets that might not have all representations, we are going to detect and add the missing columns \n",
    "    # The columns that can have types are: proto and state: need to check if all representations are done \n",
    "    state_cols = [col for col in data_features if col.startswith('state_')]\n",
    "    proto_cols = [col for col in data_features if col.startswith('proto_')]\n",
    "    \n",
    "    # Check if all columns are present\n",
    "    if 'state_PAR' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_PAR', auxCol, True)\n",
    "    if 'state_ACC' not in state_cols: \n",
    "        data_features.insert(data_features.shape[1], 'state_ACC', auxCol, True)\n",
    "    if 'state_ECO' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_ECO', auxCol, True)\n",
    "    if 'state_CON' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_CON', auxCol, True)\n",
    "    if 'state_FIN' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_FIN', auxCol, True)\n",
    "    if 'state_INT' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_INT', auxCol, True)\n",
    "    if 'state_REQ' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_REQ', auxCol, True)\n",
    "    if 'state_RST' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_RST', auxCol, True)\n",
    "    if 'proto_igmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_igmp', auxCol, True)\n",
    "    if 'proto_arp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_arp', auxCol, True)\n",
    "    if 'proto_icmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_icmp', auxCol, True)\n",
    "    if 'proto_udp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_udp', auxCol, True)\n",
    "    if 'proto_tcp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_tcp', auxCol, True)\n",
    "\n",
    "    # Normalize all data features\n",
    "    data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "    #Add dimension to data features\n",
    "    data_features = np.expand_dims(data_features, axis=2)\n",
    "    data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "    x = data_features\n",
    "    y = data_labels.to_numpy()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_shape = (24, 1, 1)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 2), padding='same'))\n",
    "    model.add(layers.Conv2D(filters=64,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 2), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(444, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 5 # Number of nodes defined for all cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training extracted and adapted from https://github.com/polvalls9/Transfer-Learning-Based-Intrusion-Detection-in-5G-and-IoT-Networks.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation function: AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(w_list): \n",
    "    \"\"\"\n",
    "    Aggregation function: Average of weights list.\n",
    "\n",
    "    Args:\n",
    "    w_list: Contains model weights of each local model\n",
    "\n",
    "    Returns:\n",
    "    avg_w: Only one set of weights which consists of the weight average. \n",
    "    \"\"\"\n",
    "    \n",
    "    avg_w = np.mean(w_list, axis=0)\n",
    "    return avg_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation function: GEOMETRIC MEDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weiszfeld_update(points, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Compute the geometric median using Weiszfeld's algorithm.\n",
    "\n",
    "    It minimizes the sum of Euclidean distances to set of points. \n",
    "    Iteratively updates estimate of median until convergence. \n",
    "    Args:\n",
    "        points: A list of points (numpy arrays) \n",
    "        tol: Tolerance for stopping criterion\n",
    "        max_iter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        median: The geometric median of the points\n",
    "    \"\"\"\n",
    "    points = np.array(points) # Points should be in numpy array \n",
    "    median = np.mean(points, axis=0) # The median is init to mean of points\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Compute distances from current median to all points\n",
    "        distances = np.linalg.norm(points - median, axis=1)\n",
    "        nonzero_distances = np.where(distances != 0, distances, np.finfo(float).eps) # Replace zero distances with small val \n",
    "        weights = 1 / nonzero_distances \n",
    "        new_median = np.sum(points * weights[:, None], axis=0) / np.sum(weights) # Weighted sum of points normalized by sum of weights\n",
    "\n",
    "        if np.linalg.norm(new_median - median) < tol: # Check convergence \n",
    "            return new_median\n",
    "\n",
    "        median = new_median # Update median for next it\n",
    "\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(w_list):\n",
    "    \"\"\"\n",
    "    Aggregates list of local weights using geometric median. \n",
    "    \n",
    "\n",
    "    Args:\n",
    "        w_list: Contains list of local model weights\n",
    "\n",
    "    Returns:\n",
    "        geomed_weights: Aggregated weights \n",
    "    \"\"\"\n",
    "    # Flatten weights to 1D vector to compute geometric median\n",
    "    flat_weights = [np.concatenate([w.flatten() for w in weights], axis=0) for weights in w_list]\n",
    "\n",
    "    # Compute geometric median using Weiszfeld's algorithm\n",
    "    flat_median = weiszfeld_update(flat_weights)\n",
    "\n",
    "    # Reshape the flat median back to the original shape\n",
    "    geomed_weights = [] # Init of return weight list \n",
    "    index = 0\n",
    "    for weight in w_list[0]:\n",
    "        shape = weight.shape \n",
    "        size = np.prod(shape)\n",
    "        geomed_weights.append(flat_median[index:index + size].reshape(shape))\n",
    "        index += size\n",
    "\n",
    "    return geomed_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation function: MEDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(w_list):\n",
    "    \"\"\"\n",
    "    Computes median of local model weights. \n",
    "\n",
    "    Args:\n",
    "        w_list: Contains model weights of each local model\n",
    "\n",
    "    Returns:\n",
    "        med_w: Median of weights for the global model \n",
    "    \"\"\"\n",
    "    \n",
    "    med_w = [np.median(layer_weights, axis=0) for layer_weights in zip(*w_list)]\n",
    "    return med_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation function: KRUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate(w_list, num_mal=0):\n",
    "    \"\"\"\n",
    "    Krum aggregation function. Method designed to robustly aggregate weights, avoiding the \n",
    "    influence of malicious nodes. It selects a set of weights closest to the majority of other nodes.\n",
    "    Uses pairwise distances. \n",
    "\n",
    "    Args:\n",
    "        w_list: Contains model weights of each local model\n",
    "        num_mal: Number of malicious nodes. Default: 0 \n",
    "\n",
    "    Returns:\n",
    "        kr_weights: Selected weights after Krum aggregation mechanism. Used for global model \n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = len(w_list) # Total number of nodes \n",
    "    num_to_consider = num_nodes - num_mal - 2 # Number of closest nodes to consider for computing Krum scores\n",
    "\n",
    "    # Flatten weights to 1D array to compute distances\n",
    "    flat_weights = [np.concatenate([w.flatten() for w in weights], axis=0) for weights in w_list]\n",
    "\n",
    "    # Matrix to store pairwise squared Euclidean distances\n",
    "    distances = np.zeros((num_nodes, num_nodes))\n",
    "    \n",
    "    # Compute pairwise squared Euclidean distances\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            dist = np.sum((flat_weights[i] - flat_weights[j]) ** 2)\n",
    "            distances[i, j] = dist\n",
    "            distances[j, i] = dist\n",
    "\n",
    "    # Krum scores for each node \n",
    "    krum_scores = []\n",
    "    for i in range(num_nodes):\n",
    "        sorted_distances = np.sort(distances[i]) # Sort distances from node to other nodes in ascending order\n",
    "        score = np.sum(sorted_distances[:num_to_consider]) # Sum distances to closest num_to_consider nodes\n",
    "        krum_scores.append(score)\n",
    "    \n",
    "    krum_index = np.argmin(krum_scores) # Select index of node with min Krum score\n",
    "    kr_weights = w_list[krum_index] # Weights from node with lowest Krum score as aggregated weihts\n",
    "\n",
    "    return kr_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING MODEL\n",
    "Execute only aggregation function of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainining datasets are defined based on the dataset partition that we want to train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1 = pd.read_csv('../x_part1.csv')\n",
    "training2 = pd.read_csv('../x_part2.csv')\n",
    "training3 = pd.read_csv('../x_part3.csv')\n",
    "training4 = pd.read_csv('../x_part4.csv')\n",
    "training5 = pd.read_csv('../x_part5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_updates = 5 # Change according to how many iterations wanted\n",
    "local_epochs = 1 # Change according to how many epochs wanted \n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train, xval, yval): \n",
    "    filepath = '../models/node'+str(node)+'w_x_5_1.keras'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping( \n",
    "                monitor = 'val_loss', \n",
    "                patience = 2 \n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_data=(xval, yval), callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize the global model and for each local dataset split into training and validation subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, val1 = train_test_split(training1, test_size=0.2, shuffle = True, random_state=42)\n",
    "train2, val2 = train_test_split(training2, test_size=0.2, shuffle = True, random_state=42)\n",
    "train3, val3 = train_test_split(training3, test_size=0.2, shuffle = True, random_state=42)\n",
    "train4, val4 = train_test_split(training4, test_size=0.2, shuffle = True, random_state=42)\n",
    "train5, val5 = train_test_split(training5, test_size=0.2, shuffle = True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all subset samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(train1)\n",
    "x2, y2 = preprocessing(train2)\n",
    "x3, y3 = preprocessing(train3)\n",
    "x4, y4 = preprocessing(train4)\n",
    "x5, y5 = preprocessing(train5)\n",
    "xv1, yv1 = preprocessing(val1)\n",
    "xv2, yv2 = preprocessing(val2)\n",
    "xv3, yv3 = preprocessing(val3)\n",
    "xv4, yv4 = preprocessing(val4)\n",
    "xv5, yv5 = preprocessing(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of local models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp0 = build_model()\n",
    "cp1 = build_model()\n",
    "cp2 = build_model()\n",
    "cp3 = build_model()\n",
    "cp4 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "for i in range(global_updates): \n",
    "    w_list = []\n",
    "    for node in range(num_nodes): \n",
    "        if node == 0: \n",
    "            x, y = x1, y1\n",
    "            xv, yv = xv1, yv1\n",
    "            cp0.set_weights(global_model.get_weights())\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "            xv, yv = xv2, yv2\n",
    "            cp1.set_weights(global_model.get_weights())\n",
    "        elif node == 2:\n",
    "            x, y = x3, y3\n",
    "            xv, yv = xv3, yv3\n",
    "            cp2.set_weights(global_model.get_weights())\n",
    "        elif node == 3:\n",
    "            x, y = x4, y4\n",
    "            xv, yv = xv4, yv4\n",
    "            cp3.set_weights(global_model.get_weights())\n",
    "        elif node == 4: \n",
    "            x, y = x5, y5\n",
    "            xv, yv = xv5, yv5\n",
    "            cp4.set_weights(global_model.get_weights())\n",
    "        cp = [cp0, cp1, cp2, cp3, cp4]\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp[node], node, x, y, xv, yv)\n",
    "        w_list.append(local_model.get_weights()) # get local weights \n",
    "\n",
    "    avg_w = aggregate(w_list) # aggregate all local weights for this iteration \n",
    "    global_model.set_weights(avg_w) # apply weights to global model \n",
    "\n",
    "\n",
    "global_model.save('../models/w_x_5_1.hdf5') # save global model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
