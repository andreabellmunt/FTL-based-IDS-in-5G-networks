{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Disable warns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW-NB15-Train-Basic\n",
    "data = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic.csv', low_memory=False)\n",
    "\n",
    "# Select the 'proto' and 'state' values that I want\n",
    "data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] =='PAR'), :]\n",
    "\n",
    "# Extracting dataset labels\n",
    "data_labels=data[['label']]\n",
    "\n",
    "# Drop the invalid features and select interested data features\n",
    "data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "\"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "# Preprocess IP and ports features\n",
    "# IP Source Address\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# IP Destination Address\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# Ports\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "# Convert all ports with 0 decimal, and HEX to DEC\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "# Convert field to int format\n",
    "data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "data_features['sport'] = data_features['sport'].astype(int)\n",
    "data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "# Convert some fields to logarithmic\n",
    "log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "for col in log1p_col:\n",
    "    data_features[col] = data_features[col].apply(np.log1p)\n",
    "    \n",
    "# Create a complementary field of attack & Transform to One hot encoding - LABELS\n",
    "normal=data_labels['label']\n",
    "normal=normal.replace(1,2)\n",
    "normal=normal.replace(0,1)\n",
    "normal=normal.replace(2,0)\n",
    "\n",
    "# Insert the new column in data labels\n",
    "data_labels.insert(1, 'normal', normal)\n",
    "data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "# Transform to One hot encoding - FEATURES\n",
    "data_features=pd.get_dummies(data_features)\n",
    "\n",
    "# Normalize all data features\n",
    "data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "#Add dimension to data features\n",
    "data_features = np.expand_dims(data_features, axis=2)\n",
    "data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "x_train, y_train=data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW-NB15-Test-Basic\n",
    "data = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv', low_memory=False)\n",
    "\n",
    "# Select the 'proto' and 'state' values that I want\n",
    "data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] =='PAR'), :]\n",
    "\n",
    "# Extracting dataset labels\n",
    "data_labels=data[['label']]\n",
    "\n",
    "#Extracting dataset features\n",
    "data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "\"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "# Preprocess IP and ports features\n",
    "# IP Source Address\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# IP Destination Address\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# Ports\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "# Convert all ports with 0 decimal, and HEX to DEC\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "# Convert field to int format\n",
    "data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "data_features['sport'] = data_features['sport'].astype(int)\n",
    "data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "# Convert some fields to logarithmic\n",
    "log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "for col in log1p_col:\n",
    "    data_features[col] = data_features[col].apply(np.log1p)\n",
    "    \n",
    "# Create a complementary field of attack & Transform to One hot encoding - LABELS\n",
    "normal=data_labels['label']\n",
    "normal=normal.replace(1,2)\n",
    "normal=normal.replace(0,1)\n",
    "normal=normal.replace(2,0)\n",
    "\n",
    "# Insert the new column in data labels\n",
    "data_labels.insert(1, 'normal', normal)\n",
    "data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "# Transform to One hot encoding - FEATURES\n",
    "data_features=pd.get_dummies(data_features)\n",
    "\n",
    "# Generate 2 new columns to fit with training\n",
    "auxCol=data_features['sbytes']\n",
    "auxCol=0\n",
    "data_features.insert(13, 'proto_igmp', auxCol, True)\n",
    "data_features.insert(21, 'state_PAR', auxCol, True)\n",
    "\n",
    "# Normalize all data features\n",
    "data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "#Add dimension to data features\n",
    "data_features = np.expand_dims(data_features, axis=2)\n",
    "data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "x_test, y_test=data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 1, 32)         352       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 1, 32)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 1, 64)         20544     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 1, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 444)               682428    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 890       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 704,214\n",
      "Trainable params: 704,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model building and definition\n",
    "input_shape = (24,1,1)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "model.add(layers.Conv2D(filters=64,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(444, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "filepath = 'C:/Users/UX430/Documents/thesis/models/1_Train-Basic_Test-Basic_Detection.hdf5' # define where the model is saved\n",
    "\n",
    "callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 10 # Stop after 10 steps with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True)]# Only save model if it is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136916, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure model training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15, batch_size=2048, callbacks=callbacks)\n",
    "history1=history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
