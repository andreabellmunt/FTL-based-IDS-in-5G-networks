{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning\n",
    "\n",
    "There are many examples all over the Internet regarding the use of PyTorch in Federated Learning architectures.\n",
    "\n",
    "The idea is to use a centralized federated learning, in which a central server organizes the training process. They are responsible for choosing the nodes and aggregating the parameters.\n",
    "\n",
    "Functions: Train clients models, aggregate parameters, test global model, set clients.\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-3-ae2103c40c22\n",
    "\n",
    "https://www.kaggle.com/code/puru98/federated-learning-pytorch\n",
    "\n",
    "https://github.com/yonetaniryo/federated_learning_pytorch/blob/master/FL_pytorch.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Disable warns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is based from the preprocessing process done by Pol Valls in his Bachelor's thesis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_14128/892409204.py:2: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part1.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_14128/892409204.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_14128/892409204.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part3.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_14128/892409204.py:5: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-A-Part3.csv')\n",
    "test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n",
    "test_plus = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "\n",
    "    # Select the 'proto' and 'state' values that I want\n",
    "    data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "    data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] == 'PAR'), :]\n",
    "\n",
    "    # Creating categories dataframe\n",
    "    data_labels = pd.DataFrame()\n",
    "\n",
    "    # Drop the invalid features and select interested data features\n",
    "    data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "    \"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "\n",
    "    # Preprocess IP and ports features\n",
    "    # IP Source Address\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "\n",
    "    # IP Destination Address\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "    # Ports\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "    # Convert all ports with 0 decimal, and HEX to DEC\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    # Convert field to int format\n",
    "    data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "    data_features['sport'] = data_features['sport'].astype(int)\n",
    "    data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "    data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "    # Convert some fields to logarithmic\n",
    "    log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "    for col in log1p_col:\n",
    "        data_features[col] = data_features[col].apply(np.log1p)\n",
    "        \n",
    "    # Transform to One Hot Encoding the Categories - normal, dos, reconnaissance, generic, exploits, worms, fuzzers, analysis, backdoor, shellcode\n",
    "    data_labels.insert(0, 'normal', data['attack_cat'].replace('normal', 1).replace(['dos', 'reconnaissance', 'generic', 'exploits', 'worms', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(1, 'dos', data['attack_cat'].replace('dos', 1).replace(['normal', 'reconnaissance', 'generic', 'exploits', 'worms', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(2, 'reconnaissance', data['attack_cat'].replace('reconnaissance', 1).replace(['normal', 'dos', 'generic', 'exploits', 'worms', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(3, 'generic', data['attack_cat'].replace('generic', 1).replace(['normal', 'dos', 'reconnaissance', 'exploits', 'worms', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(4, 'exploits', data['attack_cat'].replace('exploits', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'worms', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(5, 'worms', data['attack_cat'].replace('worms', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'exploits', 'fuzzers', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(6, 'fuzzers', data['attack_cat'].replace('fuzzers', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'exploits', 'worms', 'analysis', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(7, 'analysis', data['attack_cat'].replace('analysis', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'exploits', 'worms', 'fuzzers', 'backdoor', 'shellcode'], 0))\n",
    "    data_labels.insert(8, 'backdoor', data['attack_cat'].replace('backdoor', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'exploits', 'worms', 'fuzzers', 'analysis', 'shellcode'], 0))\n",
    "    data_labels.insert(9, 'shellcode', data['attack_cat'].replace('shellcode', 1).replace(['normal', 'dos', 'reconnaissance', 'generic', 'exploits', 'worms', 'fuzzers', 'analysis', 'backdoor'], 0))\n",
    "\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    # Transform to One hot encoding - FEATURES\n",
    "    data_features=pd.get_dummies(data_features)\n",
    "\n",
    "    # Generate 2 new columns to fit with training\n",
    "    auxCol=data_features['sbytes']\n",
    "    auxCol=0\n",
    "\n",
    "    # As we are using different datasets that might not have all representations, we are going to detect and add the missing columns \n",
    "    # The columns that can have types are: proto and state: need to check if all representations are done \n",
    "    state_cols = [col for col in data_features if col.startswith('state_')]\n",
    "    proto_cols = [col for col in data_features if col.startswith('proto_')]\n",
    "    \n",
    "    # Check if all columns are present\n",
    "    if 'state_PAR' not in state_cols:\n",
    "        data_features.insert(21, 'state_PAR', auxCol, True)\n",
    "    if 'state_ACC' not in state_cols: \n",
    "        data_features.insert(21, 'state_ACC', auxCol, True)\n",
    "    if 'state_ECO' not in state_cols:\n",
    "        data_features.insert(21, 'state_ECO', auxCol, True)\n",
    "    if 'state_CON' not in state_cols:\n",
    "        data_features.insert(21, 'state_CON', auxCol, True)\n",
    "    if 'state_FIN' not in state_cols:\n",
    "        data_features.insert(21, 'state_FIN', auxCol, True)\n",
    "    if 'state_INT' not in state_cols:\n",
    "        data_features.insert(21, 'state_INT', auxCol, True)\n",
    "    if 'state_REQ' not in state_cols:\n",
    "        data_features.insert(21, 'state_REQ', auxCol, True)\n",
    "    if 'state_RST' not in state_cols:\n",
    "        data_features.insert(21, 'state_RST', auxCol, True)\n",
    "    if 'proto_igmp' not in proto_cols:\n",
    "        data_features.insert(13, 'proto_igmp', auxCol, True)\n",
    "    if 'proto_arp' not in proto_cols:\n",
    "        data_features.insert(13, 'proto_arp', auxCol, True)\n",
    "    if 'proto_icmp' not in proto_cols:\n",
    "        data_features.insert(13, 'proto_icmp', auxCol, True)\n",
    "    if 'proto_udp' not in proto_cols:\n",
    "        data_features.insert(13, 'proto_udp', auxCol, True)\n",
    "    if 'proto_tcp' not in proto_cols:\n",
    "        data_features.insert(13, 'proto_tcp', auxCol, True)\n",
    "\n",
    "    # Normalize all data features\n",
    "    data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "    #Add dimension to data features\n",
    "    data_features = np.expand_dims(data_features, axis=2)\n",
    "    data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "    x = data_features\n",
    "    y = data_labels\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have performed the required steps previous to feeding the data to the model, we need to define said model. The idea is to use a CNN model for the local nodes and use averaging of gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and definition\n",
    "def build_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "    model.add(layers.Conv2D(filters=64,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(444, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 10\n",
    "\n",
    "# Define model training parameters\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'ID0.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    avg_grad = [np.mean(grads, axis = 0) for grads in zip(*grad_list)]\n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = preprocessing(test_basic)\n",
    "x_test_plus, y_test_plus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns values of loss, accuracy, f1, precision and recall of model evaluating with test dataset \n",
    "def evaluation(model, x, y): \n",
    "    loss, accuracy = model.evaluate(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    report = classification_report(y, y_pred, target_names=['normal', 'attack'], output_dict=True)\n",
    "    # Obtain f1, precision and recall from the report\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    return loss, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation purposes of the global model, a dataset with 15% of samples for each node is taken and merged into a val dataset\n",
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "\n",
    "x1_train, x1_val, y1_train, y1_val = train_test_split(x1, y1, test_size=0.15, random_state=42)\n",
    "x2_train, x2_val, y2_train, y2_val = train_test_split(x2, y2, test_size=0.15, random_state=42)\n",
    "x3_train, x3_val, y3_train, y3_val = train_test_split(x3, y3, test_size=0.15, random_state=42)\n",
    "\n",
    "x_val = np.concatenate((x1_val, x2_val, x3_val), axis=0)\n",
    "y_val = np.concatenate((y1_val, y2_val, y3_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 13s 184ms/step - loss: 0.2438 - accuracy: 0.9699 - val_loss: 2.5221 - val_accuracy: 0.4716\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 189ms/step - loss: 0.0304 - accuracy: 0.9924 - val_loss: 1.0647 - val_accuracy: 0.4756\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 182ms/step - loss: 0.0252 - accuracy: 0.9924 - val_loss: 0.7839 - val_accuracy: 0.4993\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 10s 183ms/step - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.8855 - val_accuracy: 0.4763\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 174ms/step - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.7567 - val_accuracy: 0.5138\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 163ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.1028 - val_accuracy: 0.9404\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 8s 158ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.1016 - val_accuracy: 0.9412\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 9s 177ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.1251 - val_accuracy: 0.9284\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0910 - val_accuracy: 0.9502\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 8s 159ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0962 - val_accuracy: 0.9478\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 174ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.4735 - val_accuracy: 0.7526\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 180ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.3912 - val_accuracy: 0.7996\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 190ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.2958 - val_accuracy: 0.8483\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 11s 196ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.5682 - val_accuracy: 0.7317\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 11s 195ms/step - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.4956 - val_accuracy: 0.7597\n",
      "1925/1925 [==============================] - 15s 8ms/step - loss: 0.0435 - accuracy: 0.9877\n",
      "1925/1925 [==============================] - 14s 7ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 175ms/step - loss: 4.9502 - accuracy: 0.5016 - val_loss: 0.3493 - val_accuracy: 0.9417\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 174ms/step - loss: 0.1292 - accuracy: 0.9294 - val_loss: 0.8879 - val_accuracy: 0.5980\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 162ms/step - loss: 0.0207 - accuracy: 0.9953 - val_loss: 0.8943 - val_accuracy: 0.5864\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 176ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.8667 - val_accuracy: 0.5917\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 8s 143ms/step - loss: 0.0175 - accuracy: 0.9955 - val_loss: 0.8538 - val_accuracy: 0.5935\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 173ms/step - loss: 0.0181 - accuracy: 0.9955 - val_loss: 0.1562 - val_accuracy: 0.9248\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 9s 166ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.1513 - val_accuracy: 0.9275\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 8s 154ms/step - loss: 0.0168 - accuracy: 0.9955 - val_loss: 0.1395 - val_accuracy: 0.9325\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 9s 171ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.1383 - val_accuracy: 0.9328\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 9s 170ms/step - loss: 0.0164 - accuracy: 0.9955 - val_loss: 0.1438 - val_accuracy: 0.9308\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 174ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.4427 - val_accuracy: 0.7606\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 11s 197ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.4522 - val_accuracy: 0.7573\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 11s 200ms/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 0.4417 - val_accuracy: 0.7620\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 171ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.4569 - val_accuracy: 0.7559\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 161ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.4435 - val_accuracy: 0.7619\n",
      "1925/1925 [==============================] - 13s 7ms/step - loss: 0.0543 - accuracy: 0.9857\n",
      "1925/1925 [==============================] - 11s 6ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 177ms/step - loss: 7.1228 - accuracy: 0.5089 - val_loss: 3.0282e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 8s 150ms/step - loss: 0.1241 - accuracy: 0.9774 - val_loss: 0.8448 - val_accuracy: 0.6705\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 8s 147ms/step - loss: 0.0352 - accuracy: 0.9946 - val_loss: 1.1095 - val_accuracy: 0.6166\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 8s 146ms/step - loss: 0.0244 - accuracy: 0.9950 - val_loss: 1.0127 - val_accuracy: 0.6284\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 8s 158ms/step - loss: 0.0195 - accuracy: 0.9952 - val_loss: 0.9410 - val_accuracy: 0.6396\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 9s 155ms/step - loss: 0.0183 - accuracy: 0.9953 - val_loss: 0.1317 - val_accuracy: 0.9443\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 8s 152ms/step - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.1358 - val_accuracy: 0.9421\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 8s 152ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1379 - val_accuracy: 0.9415\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 8s 152ms/step - loss: 0.0163 - accuracy: 0.9953 - val_loss: 0.1460 - val_accuracy: 0.9378\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 8s 153ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1428 - val_accuracy: 0.9390\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 9s 152ms/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 0.4084 - val_accuracy: 0.8090\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 8s 151ms/step - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.4306 - val_accuracy: 0.7995\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 8s 151ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.4138 - val_accuracy: 0.8072\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 8s 152ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.4192 - val_accuracy: 0.8040\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 8s 151ms/step - loss: 0.0150 - accuracy: 0.9957 - val_loss: 0.3886 - val_accuracy: 0.8163\n",
      "1925/1925 [==============================] - 16s 8ms/step - loss: 0.1394 - accuracy: 0.9657\n",
      "1925/1925 [==============================] - 15s 8ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 9s 149ms/step - loss: 24.7001 - accuracy: 0.4330 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 8s 145ms/step - loss: 28.5014 - accuracy: 0.4043 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 8s 145ms/step - loss: 23.4008 - accuracy: 0.4071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 166ms/step - loss: 18.6470 - accuracy: 0.4172 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 182ms/step - loss: 14.3416 - accuracy: 0.4542 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 177ms/step - loss: 12.3942 - accuracy: 0.4810 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 8s 160ms/step - loss: 8.9101 - accuracy: 0.5526 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 9s 166ms/step - loss: 6.2275 - accuracy: 0.6376 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 12s 231ms/step - loss: 4.3328 - accuracy: 0.7121 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 11s 211ms/step - loss: 3.0147 - accuracy: 0.7727 - val_loss: 6.1794e-11 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 13s 220ms/step - loss: 1.6626 - accuracy: 0.8640 - val_loss: 8.6509e-11 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 11s 208ms/step - loss: 1.3018 - accuracy: 0.8956 - val_loss: 5.2338e-10 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 11s 207ms/step - loss: 1.0636 - accuracy: 0.9165 - val_loss: 2.3011e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 10s 180ms/step - loss: 0.8963 - accuracy: 0.9320 - val_loss: 8.6248e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 177ms/step - loss: 0.7751 - accuracy: 0.9440 - val_loss: 2.7297e-08 - val_accuracy: 1.0000\n",
      "1925/1925 [==============================] - 16s 8ms/step - loss: 0.2441 - accuracy: 0.9878\n",
      "1925/1925 [==============================] - 11s 6ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 11s 191ms/step - loss: 18.5027 - accuracy: 0.6724 - val_loss: 74.7334 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 186ms/step - loss: 23.3467 - accuracy: 0.6576 - val_loss: 74.0864 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 176ms/step - loss: 22.9967 - accuracy: 0.6576 - val_loss: 73.2989 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 172ms/step - loss: 22.6779 - accuracy: 0.6576 - val_loss: 72.5831 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 193ms/step - loss: 22.3776 - accuracy: 0.6576 - val_loss: 71.9029 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 20.9270 - accuracy: 0.6704 - val_loss: 65.1691 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 11s 202ms/step - loss: 20.6979 - accuracy: 0.6704 - val_loss: 64.5124 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 11s 199ms/step - loss: 20.4755 - accuracy: 0.6704 - val_loss: 63.8616 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 10s 186ms/step - loss: 20.2537 - accuracy: 0.6704 - val_loss: 63.2116 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 20.0313 - accuracy: 0.6704 - val_loss: 62.5575 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 176ms/step - loss: 20.8595 - accuracy: 0.6571 - val_loss: 67.7539 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 178ms/step - loss: 20.5941 - accuracy: 0.6571 - val_loss: 67.1045 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 164ms/step - loss: 20.3251 - accuracy: 0.6571 - val_loss: 66.4500 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 10s 178ms/step - loss: 20.0531 - accuracy: 0.6571 - val_loss: 65.7852 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 183ms/step - loss: 19.7776 - accuracy: 0.6571 - val_loss: 65.1178 - val_accuracy: 0.0000e+00\n",
      "1925/1925 [==============================] - 14s 7ms/step - loss: 23.6377 - accuracy: 0.5319\n",
      "1925/1925 [==============================] - 9s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\UX430\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\UX430\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 12s 195ms/step - loss: 1.4393 - accuracy: 0.9315 - val_loss: 0.1274 - val_accuracy: 0.9622\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 194ms/step - loss: 0.2064 - accuracy: 0.9877 - val_loss: 0.1174 - val_accuracy: 0.9655\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 181ms/step - loss: 0.2071 - accuracy: 0.9877 - val_loss: 0.1174 - val_accuracy: 0.9655\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 167ms/step - loss: 0.2070 - accuracy: 0.9877 - val_loss: 0.1175 - val_accuracy: 0.9655\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 173ms/step - loss: 0.2069 - accuracy: 0.9877 - val_loss: 0.1176 - val_accuracy: 0.9655\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 11s 187ms/step - loss: 0.1963 - accuracy: 0.9873 - val_loss: 0.0908 - val_accuracy: 0.9890\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 10s 198ms/step - loss: 0.1962 - accuracy: 0.9873 - val_loss: 0.0908 - val_accuracy: 0.9890\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 10s 194ms/step - loss: 0.1962 - accuracy: 0.9873 - val_loss: 0.0909 - val_accuracy: 0.9890\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 10s 192ms/step - loss: 0.1961 - accuracy: 0.9873 - val_loss: 0.0909 - val_accuracy: 0.9890\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 11s 196ms/step - loss: 0.1961 - accuracy: 0.9873 - val_loss: 0.0910 - val_accuracy: 0.9890\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 168ms/step - loss: 0.1932 - accuracy: 0.9875 - val_loss: 0.1214 - val_accuracy: 0.9672\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 11s 196ms/step - loss: 0.1931 - accuracy: 0.9875 - val_loss: 0.1215 - val_accuracy: 0.9671\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 192ms/step - loss: 0.1930 - accuracy: 0.9875 - val_loss: 0.1216 - val_accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 10s 191ms/step - loss: 0.1930 - accuracy: 0.9875 - val_loss: 0.1217 - val_accuracy: 0.9669\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 180ms/step - loss: 0.1929 - accuracy: 0.9875 - val_loss: 0.1218 - val_accuracy: 0.9669\n",
      "1925/1925 [==============================] - 12s 6ms/step - loss: 0.1656 - accuracy: 0.9848\n",
      "1925/1925 [==============================] - 10s 5ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 11s 197ms/step - loss: 0.1457 - accuracy: 0.9890 - val_loss: 0.2219 - val_accuracy: 0.9270\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 174ms/step - loss: 0.1344 - accuracy: 0.9893 - val_loss: 0.2225 - val_accuracy: 0.9269\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 8s 153ms/step - loss: 0.1343 - accuracy: 0.9893 - val_loss: 0.2227 - val_accuracy: 0.9269\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 10s 185ms/step - loss: 0.1343 - accuracy: 0.9893 - val_loss: 0.2228 - val_accuracy: 0.9268\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 182ms/step - loss: 0.1342 - accuracy: 0.9893 - val_loss: 0.2230 - val_accuracy: 0.9268\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 175ms/step - loss: 0.1249 - accuracy: 0.9889 - val_loss: 0.1123 - val_accuracy: 0.9799\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 9s 166ms/step - loss: 0.1248 - accuracy: 0.9889 - val_loss: 0.1124 - val_accuracy: 0.9799\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 10s 189ms/step - loss: 0.1248 - accuracy: 0.9889 - val_loss: 0.1124 - val_accuracy: 0.9799\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 10s 197ms/step - loss: 0.1247 - accuracy: 0.9889 - val_loss: 0.1125 - val_accuracy: 0.9799\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.1247 - accuracy: 0.9889 - val_loss: 0.1126 - val_accuracy: 0.9799\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 11s 181ms/step - loss: 0.1224 - accuracy: 0.9892 - val_loss: 0.2171 - val_accuracy: 0.9320\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 183ms/step - loss: 0.1224 - accuracy: 0.9892 - val_loss: 0.2173 - val_accuracy: 0.9320\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 181ms/step - loss: 0.1223 - accuracy: 0.9892 - val_loss: 0.2175 - val_accuracy: 0.9320\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 175ms/step - loss: 0.1223 - accuracy: 0.9892 - val_loss: 0.2177 - val_accuracy: 0.9319\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 10s 176ms/step - loss: 0.1222 - accuracy: 0.9892 - val_loss: 0.2179 - val_accuracy: 0.9319\n",
      "1925/1925 [==============================] - 11s 6ms/step - loss: 0.1239 - accuracy: 0.9827\n",
      "1925/1925 [==============================] - 12s 6ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 8s 145ms/step - loss: 0.1305 - accuracy: 0.9889 - val_loss: 0.0357 - val_accuracy: 0.9894\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 8s 158ms/step - loss: 0.1306 - accuracy: 0.9889 - val_loss: 0.0355 - val_accuracy: 0.9894\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 181ms/step - loss: 0.1306 - accuracy: 0.9889 - val_loss: 0.0356 - val_accuracy: 0.9894\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 157ms/step - loss: 0.1305 - accuracy: 0.9889 - val_loss: 0.0357 - val_accuracy: 0.9894\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 163ms/step - loss: 0.1304 - accuracy: 0.9889 - val_loss: 0.0357 - val_accuracy: 0.9894\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 13s 229ms/step - loss: 0.1255 - accuracy: 0.9881 - val_loss: 0.0245 - val_accuracy: 0.9957\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 10s 187ms/step - loss: 0.1254 - accuracy: 0.9881 - val_loss: 0.0246 - val_accuracy: 0.9957\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 9s 175ms/step - loss: 0.1254 - accuracy: 0.9881 - val_loss: 0.0246 - val_accuracy: 0.9957\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 11s 200ms/step - loss: 0.1253 - accuracy: 0.9881 - val_loss: 0.0246 - val_accuracy: 0.9957\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 10s 192ms/step - loss: 0.1252 - accuracy: 0.9881 - val_loss: 0.0247 - val_accuracy: 0.9957\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 12s 201ms/step - loss: 0.1211 - accuracy: 0.9887 - val_loss: 0.0316 - val_accuracy: 0.9892\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 10s 182ms/step - loss: 0.1211 - accuracy: 0.9887 - val_loss: 0.0316 - val_accuracy: 0.9891\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 191ms/step - loss: 0.1210 - accuracy: 0.9887 - val_loss: 0.0317 - val_accuracy: 0.9891\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 174ms/step - loss: 0.1209 - accuracy: 0.9887 - val_loss: 0.0318 - val_accuracy: 0.9890\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 174ms/step - loss: 0.1209 - accuracy: 0.9887 - val_loss: 0.0319 - val_accuracy: 0.9890\n",
      "1925/1925 [==============================] - 13s 7ms/step - loss: 0.0935 - accuracy: 0.9889\n",
      "1925/1925 [==============================] - 12s 6ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 171ms/step - loss: 0.0550 - accuracy: 0.9915 - val_loss: 0.6594 - val_accuracy: 0.7731\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 158ms/step - loss: 0.0462 - accuracy: 0.9920 - val_loss: 0.6649 - val_accuracy: 0.7710\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 8s 150ms/step - loss: 0.0462 - accuracy: 0.9920 - val_loss: 0.6653 - val_accuracy: 0.7710\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 8s 149ms/step - loss: 0.0462 - accuracy: 0.9920 - val_loss: 0.6657 - val_accuracy: 0.7708\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 8s 155ms/step - loss: 0.0461 - accuracy: 0.9920 - val_loss: 0.6661 - val_accuracy: 0.7707\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 10s 172ms/step - loss: 0.0401 - accuracy: 0.9924 - val_loss: 0.1807 - val_accuracy: 0.9501\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 9s 167ms/step - loss: 0.0401 - accuracy: 0.9924 - val_loss: 0.1808 - val_accuracy: 0.9501\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 9s 168ms/step - loss: 0.0401 - accuracy: 0.9924 - val_loss: 0.1809 - val_accuracy: 0.9501\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 8s 155ms/step - loss: 0.0400 - accuracy: 0.9924 - val_loss: 0.1811 - val_accuracy: 0.9501\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 10s 180ms/step - loss: 0.0400 - accuracy: 0.9924 - val_loss: 0.1812 - val_accuracy: 0.9501\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 10s 164ms/step - loss: 0.0409 - accuracy: 0.9919 - val_loss: 0.5724 - val_accuracy: 0.8120\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 172ms/step - loss: 0.0409 - accuracy: 0.9919 - val_loss: 0.5727 - val_accuracy: 0.8120\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 10s 175ms/step - loss: 0.0408 - accuracy: 0.9920 - val_loss: 0.5732 - val_accuracy: 0.8119\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 167ms/step - loss: 0.0408 - accuracy: 0.9920 - val_loss: 0.5736 - val_accuracy: 0.8116\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 164ms/step - loss: 0.0408 - accuracy: 0.9920 - val_loss: 0.5740 - val_accuracy: 0.8115\n",
      "1925/1925 [==============================] - 10s 5ms/step - loss: 0.0657 - accuracy: 0.9861\n",
      "1925/1925 [==============================] - 10s 5ms/step\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 15s 255ms/step - loss: 1.0715 - accuracy: 0.9162 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 17s 307ms/step - loss: 1.5512 - accuracy: 0.8767 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 21s 392ms/step - loss: 1.5394 - accuracy: 0.8775 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 21s 392ms/step - loss: 1.5241 - accuracy: 0.8787 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 21s 397ms/step - loss: 1.5087 - accuracy: 0.8799 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 21s 339ms/step - loss: 1.9765 - accuracy: 0.8410 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 17s 315ms/step - loss: 1.9487 - accuracy: 0.8427 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 16s 293ms/step - loss: 1.9205 - accuracy: 0.8449 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 18s 333ms/step - loss: 1.8922 - accuracy: 0.8467 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 18s 334ms/step - loss: 1.8638 - accuracy: 0.8485 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 16s 254ms/step - loss: 1.3902 - accuracy: 0.8882 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 16s 296ms/step - loss: 1.3737 - accuracy: 0.8895 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 18s 331ms/step - loss: 1.3572 - accuracy: 0.8909 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 17s 319ms/step - loss: 1.3402 - accuracy: 0.8922 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 17s 316ms/step - loss: 1.3223 - accuracy: 0.8937 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "1925/1925 [==============================] - 14s 7ms/step - loss: 0.4502 - accuracy: 0.9689\n",
      "1925/1925 [==============================] - 11s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "best_it = 0\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        gradients = tape.gradient(loss, local_model.trainable_weights)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    global_model_it = global_model\n",
    "    global_model_it.optimizer.apply_gradients(zip(avg_grad, global_model_it.trainable_weights)) # apply gradients to global model\n",
    "    loss, accuracy, f1, precision, recall = evaluation(global_model_it, x_val, y_val) # Evaluate with validation dataset made before\n",
    "\n",
    "    loss_it.append(loss)\n",
    "    accuracy_it.append(accuracy)\n",
    "    f1_it.append(f1)\n",
    "    precision_it.append(precision)\n",
    "    recall_it.append(recall)\n",
    "\n",
    "    if accuracy_it[-1] == max(accuracy_it): # Choose iterations that has maximized accuracy as the global final model \n",
    "        global_model = global_model_it\n",
    "        best_it = i\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id0.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4279/4279 [==============================] - 38s 9ms/step - loss: 0.4430 - accuracy: 0.9686\n",
      "4279/4279 [==============================] - 39s 9ms/step\n",
      "1721/1721 [==============================] - 19s 11ms/step - loss: 0.4725 - accuracy: 0.9690\n",
      "1721/1721 [==============================] - 19s 11ms/step\n",
      "Loss for Test Basic:  0.44299638271331787  Loss for Test Plus:  0.47250837087631226\n",
      "Accuracy for Test Basic:  0.9685646891593933  Accuracy for Test Plus:  0.969011127948761\n",
      "F1 for Test Basic:  0.9685905234246859  F1 for Test Plus:  0.9690987746485017\n",
      "Precision for Test Basic:  0.9705306318099282  Precision for Test Plus:  0.9710533400714642\n",
      "Recall for Test Basic:  0.9685646673873032  Recall for Test Plus:  0.9690111167623338\n"
     ]
    }
   ],
   "source": [
    "loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, x_test, y_test)\n",
    "loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, x_test_plus, y_test_plus)\n",
    "\n",
    "print(\"Loss for Test Basic: \", loss_basic, \" Loss for Test Plus: \", loss_plus)\n",
    "print(\"Accuracy for Test Basic: \", accuracy_basic, \" Accuracy for Test Plus: \", accuracy_plus)\n",
    "print(\"F1 for Test Basic: \", f1_basic, \" F1 for Test Plus: \", f1_plus)\n",
    "print(\"Precision for Test Basic: \", precision_basic, \" Precision for Test Plus: \", precision_plus)\n",
    "print(\"Recall for Test Basic: \", recall_basic, \" Recall for Test Plus: \", recall_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9876918196678162,\n",
       " 0.9857107996940613,\n",
       " 0.9657221436500549,\n",
       " 0.9877567887306213,\n",
       " 0.5318827629089355,\n",
       " 0.9848177433013916,\n",
       " 0.98272305727005,\n",
       " 0.9888609051704407,\n",
       " 0.9860680103302002,\n",
       " 0.9689047932624817]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_3784/52016006.py:2: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-B-Part1.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_3784/52016006.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-B-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_3784/52016006.py:5: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-B-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-B-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-B-Part3.csv')\n",
    "test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n",
    "test_plus = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'ID2.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = preprocessing(test_basic)\n",
    "x_test_plus, y_test_plus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "57/57 [==============================] - 13s 211ms/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "57/57 [==============================] - 11s 195ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "57/57 [==============================] - 11s 199ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "57/57 [==============================] - 12s 204ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "57/57 [==============================] - 12s 204ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "53/53 [==============================] - 12s 218ms/step - loss: 3.4085 - accuracy: 0.7312 - val_loss: 0.9298 - val_accuracy: 0.8859\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 12s 220ms/step - loss: 0.1141 - accuracy: 0.9922 - val_loss: 0.3188 - val_accuracy: 0.8913\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 11s 205ms/step - loss: 0.0316 - accuracy: 0.9930 - val_loss: 0.2778 - val_accuracy: 0.8924\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 12s 222ms/step - loss: 0.0284 - accuracy: 0.9930 - val_loss: 0.2647 - val_accuracy: 0.8923\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 10s 194ms/step - loss: 0.0272 - accuracy: 0.9929 - val_loss: 0.2560 - val_accuracy: 0.8926\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 24, 1, 1), found shape=(None, 17, 1, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3784/458710089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mlocal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_val_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_val_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_local_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3784/2247087678.py\u001b[0m in \u001b[0;36mtrain_local_model\u001b[1;34m(model, node, x_train, y_train)\u001b[0m\n\u001b[0;32m     11\u001b[0m                 save_best_only = True)]# Only save model if it is the best\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 24, 1, 1), found shape=(None, 17, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it_basic = []\n",
    "accuracy_it_basic = []\n",
    "f1_it_basic = []\n",
    "precision_it_basic = []\n",
    "recall_it_basic = []\n",
    "\n",
    "loss_it_plus = []\n",
    "accuracy_it_plus = []\n",
    "f1_it_plus = []\n",
    "precision_it_plus = []\n",
    "recall_it_plus =  []\n",
    "\n",
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        gradients = tape.gradient(loss, local_model.trainable_weights)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    global_model.optimizer.apply_gradients(zip(avg_grad, global_model.trainable_weights)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, x_test, y_test)\n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, x_test_plus, y_test_plus)\n",
    "    # Save values of each iteration\n",
    "    loss_it_basic.append(loss_basic)\n",
    "    accuracy_it_basic.append(accuracy_basic)\n",
    "    f1_it_basic.append(f1_basic)\n",
    "    precision_it_basic.append(precision_basic)\n",
    "    recall_it_basic.append(recall_basic)\n",
    "\n",
    "    loss_it_plus.append(loss_plus)\n",
    "    accuracy_it_plus.append(accuracy_plus)\n",
    "    f1_it_plus.append(f1_plus)\n",
    "    precision_it_plus.append(precision_plus)\n",
    "    recall_it_plus.append(recall_plus)\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss for Test Basic: \", loss_it_basic, \" Loss for Test Plus: \", loss_it_plus)\n",
    "print(\"Accuracy for Test Basic: \", accuracy_it_basic, \" Accuracy for Test Plus: \", accuracy_it_plus)\n",
    "print(\"F1 for Test Basic: \", f1_it_basic, \" F1 for Test Plus: \", f1_it_plus)\n",
    "print(\"Precision for Test Basic: \", precision_it_basic, \" Precision for Test Plus: \", precision_it_plus)\n",
    "print(\"Recall for Test Basic: \", recall_it_basic, \" Recall for Test Plus: \", recall_it_plus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-C-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-C-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-C-Part3.csv')\n",
    "test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n",
    "test_plus = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'ID4.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = preprocessing(test_basic)\n",
    "x_test_plus, y_test_plus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values saved each iteration \n",
    "loss_it_basic = []\n",
    "accuracy_it_basic = []\n",
    "f1_it_basic = []\n",
    "precision_it_basic = []\n",
    "recall_it_basic = []\n",
    "\n",
    "loss_it_plus = []\n",
    "accuracy_it_plus = []\n",
    "f1_it_plus = []\n",
    "precision_it_plus = []\n",
    "recall_it_plus =  []\n",
    "\n",
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        gradients = tape.gradient(loss, local_model.trainable_weights)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    global_model.optimizer.apply_gradients(zip(avg_grad, global_model.trainable_weights)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, x_test, y_test)\n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, x_test_plus, y_test_plus)\n",
    "    # Save values of each iteration\n",
    "    loss_it_basic.append(loss_basic)\n",
    "    accuracy_it_basic.append(accuracy_basic)\n",
    "    f1_it_basic.append(f1_basic)\n",
    "    precision_it_basic.append(precision_basic)\n",
    "    recall_it_basic.append(recall_basic)\n",
    "\n",
    "    loss_it_plus.append(loss_plus)\n",
    "    accuracy_it_plus.append(accuracy_plus)\n",
    "    f1_it_plus.append(f1_plus)\n",
    "    precision_it_plus.append(precision_plus)\n",
    "    recall_it_plus.append(recall_plus)\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id4.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss for Test Basic: \", loss_it_basic, \" Loss for Test Plus: \", loss_it_plus)\n",
    "print(\"Accuracy for Test Basic: \", accuracy_it_basic, \" Accuracy for Test Plus: \", accuracy_it_plus)\n",
    "print(\"F1 for Test Basic: \", f1_it_basic, \" F1 for Test Plus: \", f1_it_plus)\n",
    "print(\"Precision for Test Basic: \", precision_it_basic, \" Precision for Test Plus: \", precision_it_plus)\n",
    "print(\"Recall for Test Basic: \", recall_it_basic, \" Recall for Test Plus: \", recall_it_plus)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
