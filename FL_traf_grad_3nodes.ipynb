{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Federated Learning for attack detection: 3 nodes sharing gradients**\n",
    "\n",
    "IDs from this file = **id6xy** (x = 0 if experiment with dataset, x = 1 if epochs & iterations, y being integer equal or greater than 0)\n",
    "\n",
    "In this file, experiments with different datasets, and number of epochs & iterations are done. The experiments are divided into sections, based on the elements being changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static elements for all experiments (execute first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Disable warns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "\n",
    "    # Select the 'proto' and 'state' values that I want\n",
    "    data = data.loc[(data['proto'] == 'tcp') | (data['proto'] =='udp') | (data['proto'] =='icmp') | (data['proto'] =='arp') | (data['proto'] =='ipv6-icmp') | (data['proto'] =='igmp') | (data['proto'] =='rarp'), :]\n",
    "    data = data.loc[(data['state'] == 'RST') | (data['state'] =='REQ') | (data['state'] =='INT') | (data['state'] =='FIN') | (data['state'] =='CON') | (data['state'] =='ECO') | (data['state'] =='ACC') | (data['state'] == 'PAR'), :]\n",
    "\n",
    "    # Extracting labels \n",
    "    data_labels = data[['label']]\n",
    "\n",
    "    # Drop the invalid features and select interested data features\n",
    "    data_features=data[['proto','srcip','sport','dstip','dsport','spkts','dpkts','sbytes','dbytes','state','stime','ltime','dur']]\n",
    "\n",
    "    \"\"\"PREPROCESSING\"\"\"\n",
    "\n",
    "\n",
    "    # Preprocess IP and ports features\n",
    "    # IP Source Address\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['srcip'] = data_features['srcip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "\n",
    "    # IP Destination Address\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\".\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: x.split(\":\")[-1])\n",
    "    data_features['dstip'] = data_features['dstip'].apply(lambda x: int(x, 16))\n",
    "\n",
    "    # Ports\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: x.replace('0x','') if \"0x\" in str(x) else x)\n",
    "\n",
    "    # Convert all ports with 0 decimal, and HEX to DEC\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['sport'] = data_features['sport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: str(x)[:-2] if str(x)[-2:] == '.0' else str(x))\n",
    "    data_features['dsport'] = data_features['dsport'].apply(lambda x: -1 if str(x).isalpha()==True else int(x,16))\n",
    "\n",
    "    # Convert field to int format\n",
    "    data_features['srcip'] = data_features['srcip'].astype(int)\n",
    "    data_features['sport'] = data_features['sport'].astype(int)\n",
    "    data_features['dstip'] = data_features['dstip'].astype(int)\n",
    "    data_features['dsport'] = data_features['dsport'].astype(int)\n",
    "\n",
    "    # Convert some fields to logarithmic\n",
    "    log1p_col = ['dur', 'sbytes', 'dbytes', 'spkts']\n",
    "\n",
    "    for col in log1p_col:\n",
    "        data_features[col] = data_features[col].apply(np.log1p)\n",
    "\n",
    "    # Create a complementary field of attack & Transform to One hot encoding - LABELS\n",
    "    normal=data_labels['label']\n",
    "    normal=normal.replace(1,2)\n",
    "    normal=normal.replace(0,1)\n",
    "    normal=normal.replace(2,0)\n",
    "\n",
    "    # Insert the new column in data labels\n",
    "    data_labels.insert(1, 'normal', normal)\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    data_labels = pd.get_dummies(data_labels)\n",
    "\n",
    "    # Transform to One hot encoding - FEATURES\n",
    "    data_features=pd.get_dummies(data_features)\n",
    "\n",
    "    # Value given for the missing columns\n",
    "    auxCol=0\n",
    "\n",
    "    # As we are using different datasets that might not have all representations, we are going to detect and add the missing columns \n",
    "    # The columns that can have types are: proto and state: need to check if all representations are done \n",
    "    state_cols = [col for col in data_features if col.startswith('state_')]\n",
    "    proto_cols = [col for col in data_features if col.startswith('proto_')]\n",
    "    \n",
    "    # Check if all columns are present\n",
    "    if 'state_PAR' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_PAR', auxCol, True)\n",
    "    if 'state_ACC' not in state_cols: \n",
    "        data_features.insert(data_features.shape[1], 'state_ACC', auxCol, True)\n",
    "    if 'state_ECO' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_ECO', auxCol, True)\n",
    "    if 'state_CON' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_CON', auxCol, True)\n",
    "    if 'state_FIN' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_FIN', auxCol, True)\n",
    "    if 'state_INT' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_INT', auxCol, True)\n",
    "    if 'state_REQ' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_REQ', auxCol, True)\n",
    "    if 'state_RST' not in state_cols:\n",
    "        data_features.insert(data_features.shape[1], 'state_RST', auxCol, True)\n",
    "    if 'proto_igmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_igmp', auxCol, True)\n",
    "    if 'proto_arp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_arp', auxCol, True)\n",
    "    if 'proto_icmp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_icmp', auxCol, True)\n",
    "    if 'proto_udp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_udp', auxCol, True)\n",
    "    if 'proto_tcp' not in proto_cols:\n",
    "        data_features.insert(data_features.shape[1], 'proto_tcp', auxCol, True)\n",
    "\n",
    "    # Normalize all data features\n",
    "    data_features = StandardScaler().fit_transform(data_features)\n",
    "\n",
    "    #Add dimension to data features\n",
    "    data_features = np.expand_dims(data_features, axis=2)\n",
    "    data_features = np.expand_dims(data_features, axis=3)\n",
    "\n",
    "    x = data_features\n",
    "    y = data_labels.to_numpy()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and definition\n",
    "def build_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=32,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "    model.add(layers.Conv2D(filters=64,  input_shape=input_shape, kernel_size=(1,10), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense(444, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns values of loss, accuracy, f1, precision and recall of model evaluating with test dataset \n",
    "def evaluation(model, x, y): \n",
    "    loss, accuracy = model.evaluate(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    report = classification_report(y, y_pred, target_names=['normal', 'attack'], output_dict=True)\n",
    "    # Obtain f1, precision and recall from the report\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    return loss, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/4261398225.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part1.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/4261398225.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/4261398225.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part3.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/4261398225.py:5: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3A-Part3.csv')\n",
    "test_basic = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test-Basic.csv')\n",
    "test_plus = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Test+.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id600.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 13s 247ms/step - loss: 0.2771 - accuracy: 0.9762 - val_loss: 3.3810 - val_accuracy: 0.2939\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 13s 249ms/step - loss: 0.0355 - accuracy: 0.9913 - val_loss: 1.2808 - val_accuracy: 0.3233\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 13s 250ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 1.1547 - val_accuracy: 0.3412\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 13s 252ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 1.3692 - val_accuracy: 0.2977\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 223ms/step - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.7375 - val_accuracy: 0.5239\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 12s 242ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.1361 - val_accuracy: 0.9190\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 219ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.1491 - val_accuracy: 0.9232\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 14s 266ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0823 - val_accuracy: 0.9603\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 236ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.0972 - val_accuracy: 0.9519\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 12s 227ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1543 - val_accuracy: 0.9275\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 11s 219ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.2981 - val_accuracy: 0.8605\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 12s 230ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.1623 - val_accuracy: 0.9222\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 226ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.2573 - val_accuracy: 0.8792\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 234ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.1270 - val_accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 223ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.2774 - val_accuracy: 0.8772\n",
      "4279/4279 [==============================] - 18s 4ms/step - loss: 0.2699 - accuracy: 0.9331\n",
      "1721/1721 [==============================] - 7s 4ms/step - loss: 0.3864 - accuracy: 0.8716\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 12s 226ms/step - loss: 0.0150 - accuracy: 0.9957 - val_loss: 0.7903 - val_accuracy: 0.6464\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 217ms/step - loss: 0.0144 - accuracy: 0.9960 - val_loss: 0.7309 - val_accuracy: 0.6673\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 228ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 0.7060 - val_accuracy: 0.6583\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 226ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.9999 - val_accuracy: 0.5902\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 220ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.8865 - val_accuracy: 0.6405\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 12s 227ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.0781 - val_accuracy: 0.9696\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 12s 241ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.1021 - val_accuracy: 0.9590\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 241ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.1523 - val_accuracy: 0.9389\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 241ms/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.1566 - val_accuracy: 0.9354\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 12s 231ms/step - loss: 0.0122 - accuracy: 0.9963 - val_loss: 0.1268 - val_accuracy: 0.9489\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 12s 238ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.1793 - val_accuracy: 0.9232\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 12s 229ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.2501 - val_accuracy: 0.8999\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 229ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.2145 - val_accuracy: 0.9133\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 11s 219ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 0.2581 - val_accuracy: 0.8970\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 225ms/step - loss: 0.0106 - accuracy: 0.9966 - val_loss: 0.2070 - val_accuracy: 0.9159\n",
      "4279/4279 [==============================] - 16s 4ms/step - loss: 0.1566 - accuracy: 0.9866\n",
      "1721/1721 [==============================] - 7s 4ms/step - loss: 0.3618 - accuracy: 0.8541\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 12s 233ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 1.0960 - val_accuracy: 0.6057\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 222ms/step - loss: 0.0120 - accuracy: 0.9968 - val_loss: 0.8491 - val_accuracy: 0.6586\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 239ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.9844 - val_accuracy: 0.6432\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 237ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 1.1247 - val_accuracy: 0.6219\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 12s 229ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.7581 - val_accuracy: 0.6732\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 11s 217ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.1389 - val_accuracy: 0.9443\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 215ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.1240 - val_accuracy: 0.9532\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 227ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.0864 - val_accuracy: 0.9703\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 244ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.2037 - val_accuracy: 0.9298\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 13s 263ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.1554 - val_accuracy: 0.9429\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 11s 223ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.2863 - val_accuracy: 0.8979\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 221ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.1525 - val_accuracy: 0.9339\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 11s 219ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.3293 - val_accuracy: 0.8775\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 11s 224ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.3176 - val_accuracy: 0.8834\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 221ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.4128 - val_accuracy: 0.8545\n",
      "4279/4279 [==============================] - 16s 4ms/step - loss: 0.2322 - accuracy: 0.9502\n",
      "1721/1721 [==============================] - 7s 4ms/step - loss: 0.7285 - accuracy: 0.7682\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id600.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[0.269886314868927, 0.38635921478271484], [0.1566026210784912, 0.36180034279823303], [0.2322198897600174, 0.7285167574882507]]\n",
      "Accuracy for iterations:  [[0.9331414699554443, 0.8716123104095459], [0.9866487383842468, 0.8541197180747986], [0.9502395391464233, 0.7682191133499146]]\n",
      "F1 for iterations:  [[0.9327762071646682, 0.8681414449368922], [0.9866519742811942, 0.8494644306326714], [0.9500586250958318, 0.7487386917147426]]\n",
      "Precision for iterations:  [[0.9372113567617012, 0.8882074512071244], [0.9866960188109921, 0.873779660181913], [0.9526364067437322, 0.8248187420742665]]\n",
      "Recall for iterations:  [[0.9331414882117502, 0.8716122938312868], [0.9866487481375442, 0.8541197413354646], [0.9502395629437027, 0.7682191382692727]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/1530530892.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3B-Part2.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3B-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3B-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3B-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id601.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 11s 211ms/step - loss: 0.1292 - accuracy: 0.9992 - val_loss: 1.7152e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 213ms/step - loss: 1.1471e-05 - accuracy: 1.0000 - val_loss: 9.7977e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 12s 226ms/step - loss: 9.3926e-06 - accuracy: 1.0000 - val_loss: 8.8217e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 12s 237ms/step - loss: 8.4525e-06 - accuracy: 1.0000 - val_loss: 7.9229e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 224ms/step - loss: 7.5693e-06 - accuracy: 1.0000 - val_loss: 7.0754e-06 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 151ms/step - loss: 1.0974 - accuracy: 0.6250 - val_loss: 0.8066 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 150ms/step - loss: 0.3303 - accuracy: 0.8723 - val_loss: 1.0311 - val_accuracy: 0.8477\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 161ms/step - loss: 0.1646 - accuracy: 0.9876 - val_loss: 0.3240 - val_accuracy: 0.8866\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.0360 - accuracy: 0.9920 - val_loss: 0.2600 - val_accuracy: 0.8873\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 161ms/step - loss: 0.0282 - accuracy: 0.9923 - val_loss: 0.2284 - val_accuracy: 0.8889\n",
      "Epoch 1/5\n",
      "36/36 [==============================] - 6s 177ms/step - loss: 0.2131 - accuracy: 0.9336 - val_loss: 8.2718e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 7s 190ms/step - loss: 5.8070e-04 - accuracy: 1.0000 - val_loss: 4.4176e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 7s 188ms/step - loss: 4.3219e-04 - accuracy: 1.0000 - val_loss: 3.6435e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 6s 168ms/step - loss: 3.3596e-04 - accuracy: 1.0000 - val_loss: 2.6757e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 6s 176ms/step - loss: 2.2762e-04 - accuracy: 1.0000 - val_loss: 1.7076e-04 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 12s 3ms/step - loss: 4.9985 - accuracy: 0.4713\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 5.2200 - accuracy: 0.4394\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 174ms/step - loss: 2.1150 - accuracy: 0.6023 - val_loss: 0.0059 - val_accuracy: 0.9971\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 9s 167ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 6.6856e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 9s 177ms/step - loss: 3.8943e-04 - accuracy: 1.0000 - val_loss: 2.1734e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 10s 192ms/step - loss: 1.5481e-04 - accuracy: 1.0000 - val_loss: 1.0529e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 11s 207ms/step - loss: 8.2292e-05 - accuracy: 1.0000 - val_loss: 6.0424e-05 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 176ms/step - loss: 0.2647 - accuracy: 0.9082 - val_loss: 0.2576 - val_accuracy: 0.8865\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 9s 169ms/step - loss: 0.0281 - accuracy: 0.9922 - val_loss: 0.1874 - val_accuracy: 0.8926\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 9s 169ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.1970 - val_accuracy: 0.8892\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 168ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.1454 - val_accuracy: 0.9035\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 166ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.1498 - val_accuracy: 0.9015\n",
      "Epoch 1/5\n",
      "36/36 [==============================] - 6s 169ms/step - loss: 0.3246 - accuracy: 0.8954 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 6s 156ms/step - loss: 4.4778e-04 - accuracy: 1.0000 - val_loss: 6.8626e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 6s 160ms/step - loss: 2.5459e-04 - accuracy: 1.0000 - val_loss: 4.6654e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 6s 161ms/step - loss: 1.7057e-04 - accuracy: 1.0000 - val_loss: 3.0332e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 6s 160ms/step - loss: 8.6025e-05 - accuracy: 1.0000 - val_loss: 7.3951e-05 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 15s 4ms/step - loss: 5.6325 - accuracy: 0.4714 0s - loss: 5\n",
      "1721/1721 [==============================] - 6s 3ms/step - loss: 5.9699 - accuracy: 0.4394: 0s - loss: 5.5814 \n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 170ms/step - loss: 2.6598 - accuracy: 0.5440 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 163ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7537e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 3.1401e-04 - accuracy: 1.0000 - val_loss: 1.9989e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 168ms/step - loss: 1.4989e-04 - accuracy: 1.0000 - val_loss: 1.0856e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 8.7075e-05 - accuracy: 1.0000 - val_loss: 6.7577e-05 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 0.1736 - accuracy: 0.9349 - val_loss: 0.2056 - val_accuracy: 0.8854\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 161ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.1453 - val_accuracy: 0.9008\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 160ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.1320 - val_accuracy: 0.9093\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.1236 - val_accuracy: 0.9143\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 160ms/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.1270 - val_accuracy: 0.9124\n",
      "Epoch 1/5\n",
      "36/36 [==============================] - 6s 163ms/step - loss: 0.3464 - accuracy: 0.8919 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 6s 159ms/step - loss: 7.3045e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 6s 164ms/step - loss: 4.0387e-04 - accuracy: 1.0000 - val_loss: 7.2766e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 6s 162ms/step - loss: 2.4864e-04 - accuracy: 1.0000 - val_loss: 3.6967e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 6s 159ms/step - loss: 1.2971e-04 - accuracy: 1.0000 - val_loss: 1.8359e-04 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 14s 3ms/step - loss: 4.8905 - accuracy: 0.4718\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 5.1775 - accuracy: 0.4397\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id601.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[4.998519420623779, 5.220028400421143], [5.632476806640625, 5.969873428344727], [4.890505790710449, 5.1775360107421875]]\n",
      "Accuracy for iterations:  [[0.471259742975235, 0.4393664300441742], [0.4713546931743622, 0.4394209086894989], [0.4717783033847809, 0.4397297203540802]]\n",
      "F1 for iterations:  [[0.30195648712973155, 0.26837947544156576], [0.3021657967644099, 0.268498556625407], [0.3030988407521974, 0.2691729649829838]]\n",
      "Precision for iterations:  [[0.7508402958425918, 0.753716401752156], [0.7508613821852271, 0.7537269185756607], [0.7509555085271363, 0.7537865355700379]]\n",
      "Recall for iterations:  [[0.47125975050395863, 0.43936641720555114], [0.4713546992316457, 0.4394209111385599], [0.4717783166320956, 0.4397297100922764]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/2150709242.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3C-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/2150709242.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3C-Part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3C-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3C-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3C-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id602.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26/26 [==============================] - 5s 174ms/step - loss: 0.3379 - accuracy: 0.9630 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.0180 - accuracy: 0.9988 - val_loss: 1.0095e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 4s 169ms/step - loss: 0.0138 - accuracy: 0.9988 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.0081 - accuracy: 0.9988 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 5s 188ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0380 - val_accuracy: 0.9986\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 164ms/step - loss: 0.5802 - accuracy: 0.8547 - val_loss: 1.3953 - val_accuracy: 0.6150\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 9s 180ms/step - loss: 0.0658 - accuracy: 0.9913 - val_loss: 1.0074 - val_accuracy: 0.6173\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 10s 192ms/step - loss: 0.0309 - accuracy: 0.9921 - val_loss: 0.7314 - val_accuracy: 0.6184\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 10s 194ms/step - loss: 0.0261 - accuracy: 0.9920 - val_loss: 0.6169 - val_accuracy: 0.6284\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 9s 178ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.5899 - val_accuracy: 0.6232\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 10s 193ms/step - loss: 0.0198 - accuracy: 0.9924 - val_loss: 0.2349 - val_accuracy: 0.8897\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 11s 208ms/step - loss: 0.0168 - accuracy: 0.9936 - val_loss: 0.3477 - val_accuracy: 0.7563\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 10s 190ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.4574 - val_accuracy: 0.7372\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 10s 188ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.4816 - val_accuracy: 0.7547\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 10s 190ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.3853 - val_accuracy: 0.8119\n",
      "4279/4279 [==============================] - 13s 3ms/step - loss: 0.0662 - accuracy: 0.9824\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.2546 - accuracy: 0.8782: 0s - loss: 0.2638 - accura\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 4s 170ms/step - loss: 2.3729 - accuracy: 0.7271 - val_loss: 0.0699 - val_accuracy: 0.9836\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0073 - val_accuracy: 0.9972\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 5s 185ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0058 - val_accuracy: 0.9972\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 4s 173ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0060 - val_accuracy: 0.9972\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 5s 181ms/step - loss: 9.6601e-04 - accuracy: 0.9999 - val_loss: 0.0065 - val_accuracy: 0.9972\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 180ms/step - loss: 1.3536 - accuracy: 0.8324 - val_loss: 1.0518 - val_accuracy: 0.6461\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 9s 175ms/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.4383 - val_accuracy: 0.7611\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 10s 190ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.4125 - val_accuracy: 0.7719\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 184ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.3952 - val_accuracy: 0.7799\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 9s 177ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.3771 - val_accuracy: 0.7895\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 171ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.3578 - val_accuracy: 0.8194\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 0.0122 - accuracy: 0.9964 - val_loss: 0.3974 - val_accuracy: 0.8152\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 164ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 0.2982 - val_accuracy: 0.8538\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 173ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.4735 - val_accuracy: 0.8079\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 9s 173ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.4648 - val_accuracy: 0.8190\n",
      "4279/4279 [==============================] - 13s 3ms/step - loss: 0.0661 - accuracy: 0.9847\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.2335 - accuracy: 0.9045\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 5s 192ms/step - loss: 3.4620 - accuracy: 0.7080 - val_loss: 0.0776 - val_accuracy: 0.9849\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 5s 174ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 0.9995\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 5s 180ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 6.2871e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 5s 174ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 6.7046e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 5s 181ms/step - loss: 9.7526e-04 - accuracy: 0.9999 - val_loss: 7.3626e-04 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 186ms/step - loss: 1.7369 - accuracy: 0.8315 - val_loss: 1.3593 - val_accuracy: 0.6555\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 10s 192ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.5831 - val_accuracy: 0.7463\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 9s 186ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.4582 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 182ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.4435 - val_accuracy: 0.7936\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 9s 179ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 0.4381 - val_accuracy: 0.7965\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 10s 199ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.4314 - val_accuracy: 0.8209\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 10s 192ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.4533 - val_accuracy: 0.8220\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 10s 190ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.3869 - val_accuracy: 0.8406\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 10s 196ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.4261 - val_accuracy: 0.8345\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 10s 192ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.3937 - val_accuracy: 0.8434\n",
      "4279/4279 [==============================] - 15s 4ms/step - loss: 0.0810 - accuracy: 0.9806\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.2162 - accuracy: 0.9216\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id602.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[0.06623820215463638, 0.2545536756515503], [0.06610514223575592, 0.2335139662027359], [0.08103546500205994, 0.21621164679527283]]\n",
      "Accuracy for iterations:  [[0.9823906421661377, 0.87822425365448], [0.9847278594970703, 0.9045266509056091], [0.9805939197540283, 0.921565055847168]]\n",
      "F1 for iterations:  [[0.9823851797095589, 0.875168327832335], [0.9847269640137685, 0.9030180032236031], [0.9805860543248166, 0.9207001222559648]]\n",
      "Precision for iterations:  [[0.9824282326610256, 0.8934943786819013], [0.9847280443740247, 0.9128781556696317], [0.9806571853604926, 0.9266130258528362]]\n",
      "Recall for iterations:  [[0.9823906628881942, 0.8782242243696868], [0.9847278623389524, 0.904526629368597], [0.980593940810424, 0.9215650657560125]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3D-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3D-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3D-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id603.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 3s 199ms/step - loss: 0.4505 - accuracy: 0.9962 - val_loss: 0.1225 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 180ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 9.9241e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 3.2011e-04 - accuracy: 1.0000 - val_loss: 6.0621e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 3.7339e-05 - accuracy: 1.0000 - val_loss: 2.2484e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 1.9396e-05 - accuracy: 1.0000 - val_loss: 1.6396e-05 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 5.5540 - accuracy: 0.0000e+00 - val_loss: 2.3688 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 1.3972 - accuracy: 0.0000e+00 - val_loss: 0.7928 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.6832 - accuracy: 0.6787 - val_loss: 0.6223 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 0.5728 - accuracy: 1.0000 - val_loss: 0.4923 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 0.3877 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 159ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.7850e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 149ms/step - loss: 2.7283e-04 - accuracy: 1.0000 - val_loss: 1.0855e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 151ms/step - loss: 1.1012e-04 - accuracy: 1.0000 - val_loss: 6.6444e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 161ms/step - loss: 7.7639e-05 - accuracy: 1.0000 - val_loss: 5.3469e-05 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 12s 3ms/step - loss: 6.3781 - accuracy: 0.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721/1721 [==============================] - 5s 3ms/step - loss: 6.6773 - accuracy: 0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 7.6749 - accuracy: 0.0000e+00 - val_loss: 3.8253 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 2.1286 - accuracy: 1.5063e-04 - val_loss: 0.9624 - val_accuracy: 0.0011\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 154ms/step - loss: 0.6069 - accuracy: 0.6554 - val_loss: 0.2978 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 152ms/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 161ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 188ms/step - loss: 1.6333 - accuracy: 0.0000e+00 - val_loss: 0.7891 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.5555 - accuracy: 0.8371 - val_loss: 0.3660 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 149ms/step - loss: 0.2870 - accuracy: 1.0000 - val_loss: 0.2087 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 0.1563 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 0.0719 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 161ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 5.9423e-04 - accuracy: 1.0000 - val_loss: 7.5466e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 150ms/step - loss: 4.2715e-05 - accuracy: 1.0000 - val_loss: 1.3788e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 149ms/step - loss: 1.1674e-05 - accuracy: 1.0000 - val_loss: 6.4803e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 6.7444e-06 - accuracy: 1.0000 - val_loss: 4.5594e-06 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 11s 2ms/step - loss: 7.5728 - accuracy: 0.4712\n",
      "   1/1721 [..............................] - ETA: 0s - loss: 1.2219e-06 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721/1721 [==============================] - 4s 2ms/step - loss: 7.9657 - accuracy: 0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 154ms/step - loss: 9.1337 - accuracy: 0.0000e+00 - val_loss: 5.1446 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 154ms/step - loss: 2.6841 - accuracy: 0.1114 - val_loss: 0.2995 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 147ms/step - loss: 6.8383e-04 - accuracy: 1.0000 - val_loss: 2.3278e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 152ms/step - loss: 1.7309e-04 - accuracy: 1.0000 - val_loss: 1.2766e-04 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 187ms/step - loss: 2.9450 - accuracy: 0.1630 - val_loss: 0.2792 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.0924 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 152ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 147ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 6.6372e-04 - accuracy: 1.0000 - val_loss: 4.3302e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 2.1715e-05 - accuracy: 1.0000 - val_loss: 5.6568e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 4.5154e-06 - accuracy: 1.0000 - val_loss: 2.4373e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 2s 149ms/step - loss: 2.4736e-06 - accuracy: 1.0000 - val_loss: 1.7107e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 1.9086e-06 - accuracy: 1.0000 - val_loss: 1.4276e-06 - val_accuracy: 1.0000\n",
      "4279/4279 [==============================] - 11s 3ms/step - loss: 7.9412 - accuracy: 0.4712\n",
      "   1/1721 [..............................] - ETA: 0s - loss: 5.2154e-07 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1721/1721 [==============================] - 6s 3ms/step - loss: 8.3740 - accuracy: 0.4392: 0s - loss: 7.8252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\UX430\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id603.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[6.378147125244141, 6.677319049835205], [7.572839260101318, 7.96567964553833], [7.941164493560791, 8.37397575378418]]\n",
      "Accuracy for iterations:  [[0.4712086319923401, 0.4392392635345459], [0.4712086319923401, 0.4392392635345459], [0.4712086319923401, 0.4392392635345459]]\n",
      "F1 for iterations:  [[0.30184375474744357, 0.2681015400046588], [0.30184375474744357, 0.2681015400046588], [0.30184375474744357, 0.2681015400046588]]\n",
      "Precision for iterations:  [[0.2220375675826312, 0.19293113164997755], [0.2220375675826312, 0.19293113164997755], [0.2220375675826312, 0.19293113164997755]]\n",
      "Recall for iterations:  [[0.4712086242659733, 0.43923926469519725], [0.4712086242659733, 0.43923926469519725], [0.4712086242659733, 0.43923926469519725]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET 3E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/3505167362.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3E-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/3505167362.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3E-Part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3E-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3E-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3E-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id604.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.2821 - accuracy: 0.9717 - val_loss: 1.9636 - val_accuracy: 0.6682\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 157ms/step - loss: 0.0366 - accuracy: 0.9921 - val_loss: 0.7501 - val_accuracy: 0.6713\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 162ms/step - loss: 0.0268 - accuracy: 0.9923 - val_loss: 0.6690 - val_accuracy: 0.6736\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.5127 - val_accuracy: 0.6952\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 157ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.4690 - val_accuracy: 0.7125\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 164ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.3507 - val_accuracy: 0.7816\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 157ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.6406 - val_accuracy: 0.6583\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.4124 - val_accuracy: 0.7691\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.6378 - val_accuracy: 0.7136\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.5523 - val_accuracy: 0.7522\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.4560 - val_accuracy: 0.7922\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.5261 - val_accuracy: 0.7853\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.5863 - val_accuracy: 0.7691\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 152ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.4640 - val_accuracy: 0.8033\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 152ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.3327 - val_accuracy: 0.8338\n",
      "4279/4279 [==============================] - 11s 3ms/step - loss: 0.1049 - accuracy: 0.9804\n",
      "1721/1721 [==============================] - 6s 3ms/step - loss: 0.2577 - accuracy: 0.9184\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.3641 - val_accuracy: 0.8272\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 0.3296 - val_accuracy: 0.8452\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 161ms/step - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.3104 - val_accuracy: 0.8526\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0140 - accuracy: 0.9962 - val_loss: 0.3644 - val_accuracy: 0.8243\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 152ms/step - loss: 0.0141 - accuracy: 0.9962 - val_loss: 0.3394 - val_accuracy: 0.8379\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.7770 - val_accuracy: 0.7089\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.4614 - val_accuracy: 0.8032\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.5794 - val_accuracy: 0.7817\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 157ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.3693 - val_accuracy: 0.8313\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 152ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.6310 - val_accuracy: 0.7710\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.5404 - val_accuracy: 0.8118\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 164ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.4615 - val_accuracy: 0.8270\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.6567 - val_accuracy: 0.7845\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.6273 - val_accuracy: 0.7868\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.5681 - val_accuracy: 0.8038\n",
      "4279/4279 [==============================] - 11s 3ms/step - loss: 0.1131 - accuracy: 0.9869 0s -\n",
      "1721/1721 [==============================] - 6s 3ms/step - loss: 0.3692 - accuracy: 0.8813: 0s - loss: 0.3746 - accuracy: 0.\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.5292 - val_accuracy: 0.7956\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.0122 - accuracy: 0.9968 - val_loss: 0.3028 - val_accuracy: 0.8784\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 162ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.2744 - val_accuracy: 0.8895\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 161ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.2052 - val_accuracy: 0.9174\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 160ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.3054 - val_accuracy: 0.8754\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.7592 - val_accuracy: 0.7488\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 160ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.9243 - val_accuracy: 0.7313\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.6197 - val_accuracy: 0.7959\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 8s 158ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 0.8561 - val_accuracy: 0.7413\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 8s 165ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.8131 - val_accuracy: 0.7460\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 9s 171ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.7895 - val_accuracy: 0.7733\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 8s 166ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.6716 - val_accuracy: 0.7919\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 9s 169ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.6322 - val_accuracy: 0.7971\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 9s 175ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.9487 - val_accuracy: 0.7482\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 9s 170ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.6548 - val_accuracy: 0.7928\n",
      "4279/4279 [==============================] - 12s 3ms/step - loss: 0.1571 - accuracy: 0.9715\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.5682 - accuracy: 0.8097\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id604.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[0.10486697405576706, 0.257686972618103], [0.11309099942445755, 0.36915287375450134], [0.1571233868598938, 0.5682361721992493]]\n",
      "Accuracy for iterations:  [[0.9804259538650513, 0.9184407591819763], [0.9868605732917786, 0.8812577128410339], [0.9714715480804443, 0.8096890449523926]]\n",
      "F1 for iterations:  [[0.9804387972989596, 0.9178054025404584], [0.9868637923234811, 0.8786637507403421], [0.9714436948455951, 0.7995370116182215]]\n",
      "Precision for iterations:  [[0.9809291872122027, 0.9211076046577489], [0.9869095786733787, 0.8937045435044499], [0.9717799989701116, 0.8443418509629739]]\n",
      "Recall for iterations:  [[0.9804259545999007, 0.9184407469301751], [0.9868605568377692, 0.881257719973843], [0.9714715592041836, 0.8096890212889631]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET 3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/89903324.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part1.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/89903324.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/89903324.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3F-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id605.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31/31 [==============================] - 5s 163ms/step - loss: 0.4862 - accuracy: 0.7824 - val_loss: 0.8133 - val_accuracy: 0.3810\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 5s 165ms/step - loss: 0.1971 - accuracy: 0.9281 - val_loss: 0.1752 - val_accuracy: 0.9654\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 5s 161ms/step - loss: 0.0932 - accuracy: 0.9769 - val_loss: 0.1095 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 5s 162ms/step - loss: 0.0620 - accuracy: 0.9845 - val_loss: 0.0872 - val_accuracy: 0.9960\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 5s 161ms/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.0772 - val_accuracy: 0.9981\n",
      "Epoch 1/5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.0198 - val_accuracy: 0.9898\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 17s 170ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.0143 - val_accuracy: 0.9938\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.0118 - val_accuracy: 0.9948\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 16s 167ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0116 - val_accuracy: 0.9949\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 16s 163ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.0160 - val_accuracy: 0.9932\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 6s 172ms/step - loss: 0.0568 - accuracy: 0.9782 - val_loss: 0.2185 - val_accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 6s 182ms/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.2119 - val_accuracy: 0.8928\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.2132 - val_accuracy: 0.8876\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 6s 173ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.2506 - val_accuracy: 0.8645\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.1574 - val_accuracy: 0.9222\n",
      "4279/4279 [==============================] - 14s 3ms/step - loss: 0.0360 - accuracy: 0.9905\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.1180 - accuracy: 0.9646: 0s - l\n",
      "Epoch 1/5\n",
      "31/31 [==============================] - 5s 162ms/step - loss: 0.0560 - accuracy: 0.9839 - val_loss: 0.0816 - val_accuracy: 0.9970\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 5s 159ms/step - loss: 0.0390 - accuracy: 0.9885 - val_loss: 0.1069 - val_accuracy: 0.9977\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 5s 164ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 0.1101 - val_accuracy: 0.9975\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 5s 159ms/step - loss: 0.0350 - accuracy: 0.9886 - val_loss: 0.1264 - val_accuracy: 0.9963\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 5s 157ms/step - loss: 0.0341 - accuracy: 0.9886 - val_loss: 0.1616 - val_accuracy: 0.9945\n",
      "Epoch 1/5\n",
      "98/98 [==============================] - 15s 158ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.0149 - val_accuracy: 0.9934\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 15s 157ms/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.0171 - val_accuracy: 0.9934\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.0140 - val_accuracy: 0.9942\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 17s 170ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.0091 - val_accuracy: 0.9970\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 17s 172ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.0160 - val_accuracy: 0.9944\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 0.0290 - accuracy: 0.9914 - val_loss: 0.2570 - val_accuracy: 0.8774\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 6s 172ms/step - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.1583 - val_accuracy: 0.9226\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.1993 - val_accuracy: 0.9046\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 6s 185ms/step - loss: 0.0122 - accuracy: 0.9956 - val_loss: 0.1938 - val_accuracy: 0.9078\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.3082 - val_accuracy: 0.8669\n",
      "4279/4279 [==============================] - 13s 3ms/step - loss: 0.0596 - accuracy: 0.9711\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.2758 - accuracy: 0.8912\n",
      "Epoch 1/5\n",
      "31/31 [==============================] - 6s 199ms/step - loss: 0.0504 - accuracy: 0.9852 - val_loss: 0.0882 - val_accuracy: 0.9979\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 7s 210ms/step - loss: 0.0349 - accuracy: 0.9887 - val_loss: 0.1563 - val_accuracy: 0.9948\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0325 - accuracy: 0.9889 - val_loss: 0.1427 - val_accuracy: 0.9953\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.2465 - val_accuracy: 0.9339\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 6s 200ms/step - loss: 0.0314 - accuracy: 0.9894 - val_loss: 0.1866 - val_accuracy: 0.9841\n",
      "Epoch 1/5\n",
      "98/98 [==============================] - 17s 169ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.0194 - val_accuracy: 0.9934\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 17s 174ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.0123 - val_accuracy: 0.9963\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 17s 174ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0151 - val_accuracy: 0.9951\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 17s 171ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0157 - val_accuracy: 0.9948\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 17s 173ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0136 - val_accuracy: 0.9952\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 6s 174ms/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.3935 - val_accuracy: 0.8574\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 5s 163ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.1888 - val_accuracy: 0.9133\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.2567 - val_accuracy: 0.8994\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 5s 161ms/step - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.1342 - val_accuracy: 0.9361\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 5s 164ms/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 0.4269 - val_accuracy: 0.8372\n",
      "4279/4279 [==============================] - 12s 3ms/step - loss: 0.0890 - accuracy: 0.9673\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.5813 - accuracy: 0.7663\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id605.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[0.035965338349342346, 0.11802816390991211], [0.05962184816598892, 0.2757599651813507], [0.08898474276065826, 0.581328809261322]]\n",
      "Accuracy for iterations:  [[0.9904686212539673, 0.9646152853965759], [0.9711356163024902, 0.8912482857704163], [0.9672937989234924, 0.7662573456764221]]\n",
      "F1 for iterations:  [[0.9904723452407961, 0.9645994327783397], [0.9710852062318768, 0.8891216495116585], [0.9672131450905261, 0.7461594868219816]]\n",
      "Precision for iterations:  [[0.9905988643460443, 0.9646079241040298], [0.9719703032510495, 0.902236437988811], [0.9687188040919812, 0.8243363146066571]]\n",
      "Recall for iterations:  [[0.9904686084898771, 0.9646152728329579], [0.9711355867831372, 0.8912482743587881], [0.9672938151859535, 0.7662573566809562]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET 3G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/1702362103.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3G-Part2.csv')\n",
      "C:\\Users\\UX430\\AppData\\Local\\Temp/ipykernel_8972/1702362103.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3G-Part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3G-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3G-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt3G-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 3\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id606.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 4s 225ms/step - loss: 0.5101 - accuracy: 0.9398 - val_loss: 1.7044 - val_accuracy: 0.1459\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 3s 150ms/step - loss: 0.1858 - accuracy: 0.9526 - val_loss: 1.1748 - val_accuracy: 0.2292\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.0778 - accuracy: 0.9779 - val_loss: 0.5421 - val_accuracy: 0.7749\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.0545 - accuracy: 0.9818 - val_loss: 0.5352 - val_accuracy: 0.7419\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 3s 160ms/step - loss: 0.0458 - accuracy: 0.9844 - val_loss: 0.3217 - val_accuracy: 0.9157\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 9s 160ms/step - loss: 0.0236 - accuracy: 0.9918 - val_loss: 0.3693 - val_accuracy: 0.7411\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 166ms/step - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.2382 - val_accuracy: 0.8541\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 160ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.4471 - val_accuracy: 0.7449\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 167ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.3307 - val_accuracy: 0.8189\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 168ms/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 0.2445 - val_accuracy: 0.8609\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.2254 - val_accuracy: 0.8930\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.1883 - val_accuracy: 0.9079\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.1962 - val_accuracy: 0.9072\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 13s 163ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.2296 - val_accuracy: 0.9000\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.3694 - val_accuracy: 0.8639\n",
      "4279/4279 [==============================] - 13s 3ms/step - loss: 0.0941 - accuracy: 0.9881\n",
      "1721/1721 [==============================] - 7s 4ms/step - loss: 0.3991 - accuracy: 0.8597: 2s - loss: - ETA: 2s\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 3s 156ms/step - loss: 0.0258 - accuracy: 0.9899 - val_loss: 0.3322 - val_accuracy: 0.8132\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.0228 - accuracy: 0.9915 - val_loss: 0.5188 - val_accuracy: 0.7033\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 3s 151ms/step - loss: 0.0227 - accuracy: 0.9916 - val_loss: 0.1450 - val_accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 3s 157ms/step - loss: 0.0220 - accuracy: 0.9919 - val_loss: 0.3917 - val_accuracy: 0.7685\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 3s 156ms/step - loss: 0.0215 - accuracy: 0.9919 - val_loss: 0.2512 - val_accuracy: 0.8675\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 9s 165ms/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.3871 - val_accuracy: 0.8272\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 160ms/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.4620 - val_accuracy: 0.8007\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 162ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.3325 - val_accuracy: 0.8529\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 162ms/step - loss: 0.0126 - accuracy: 0.9964 - val_loss: 0.4077 - val_accuracy: 0.8336\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 169ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.4413 - val_accuracy: 0.8339\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 13s 160ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.2394 - val_accuracy: 0.9059\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 13s 162ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.3968 - val_accuracy: 0.8736\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 13s 160ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.2142 - val_accuracy: 0.9117\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.3840 - val_accuracy: 0.8786\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.4277 - val_accuracy: 0.8662\n",
      "4279/4279 [==============================] - 12s 3ms/step - loss: 0.1344 - accuracy: 0.9766\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.9722 - accuracy: 0.7049\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 3s 154ms/step - loss: 0.0232 - accuracy: 0.9910 - val_loss: 0.5269 - val_accuracy: 0.7222\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 3s 159ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.3087 - val_accuracy: 0.8207\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 3s 152ms/step - loss: 0.0187 - accuracy: 0.9922 - val_loss: 0.4896 - val_accuracy: 0.7512\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.0180 - accuracy: 0.9926 - val_loss: 0.4972 - val_accuracy: 0.7534\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.0180 - accuracy: 0.9926 - val_loss: 0.4629 - val_accuracy: 0.7656\n",
      "Epoch 1/5\n",
      "54/54 [==============================] - 9s 160ms/step - loss: 0.0115 - accuracy: 0.9967 - val_loss: 0.3525 - val_accuracy: 0.8512\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 9s 162ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.3391 - val_accuracy: 0.8576\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 9s 160ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.4251 - val_accuracy: 0.8428\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 9s 158ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.6311 - val_accuracy: 0.7930\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 9s 163ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.4513 - val_accuracy: 0.8381\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 13s 161ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.3207 - val_accuracy: 0.8905\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 13s 162ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.5100 - val_accuracy: 0.8435\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 13s 161ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.3199 - val_accuracy: 0.8929\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.4437 - val_accuracy: 0.8647\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.3218 - val_accuracy: 0.8932\n",
      "4279/4279 [==============================] - 13s 3ms/step - loss: 0.1178 - accuracy: 0.9817\n",
      "1721/1721 [==============================] - 5s 3ms/step - loss: 0.9957 - accuracy: 0.6954\n"
     ]
    }
   ],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id606.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iterations:  [[0.09410139918327332, 0.3991059958934784], [0.13437721133232117, 0.9722410440444946], [0.11784116178750992, 0.9957458972930908]]\n",
      "Accuracy for iterations:  [[0.9881168007850647, 0.8596781492233276], [0.9765549898147583, 0.7048971652984619], [0.9817113876342773, 0.6953788995742798]]\n",
      "F1 for iterations:  [[0.9881193443176182, 0.8551147073415144], [0.9765309494289268, 0.6639681514001282], [0.9817037076793979, 0.6504287255082116]]\n",
      "Precision for iterations:  [[0.9881532751605089, 0.8805236376187426], [0.9769034960094853, 0.7931737395189035], [0.9817804950266569, 0.7874536043719509]]\n",
      "Recall for iterations:  [[0.9881168015425517, 0.8596781225023614], [0.9765549680095825, 0.7048971881130568], [0.9817114142978176, 0.6953789144808544]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with epochs and iterations \n",
    "\n",
    "Use of dataset 3C, considering it contains an equal partition of the data between the 3 nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Epochs = 10, #Iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 10\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id610.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id610.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Epochs = 5, #Iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets and testing datasets \n",
    "training1 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part1.csv')\n",
    "training2 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part2.csv')\n",
    "training3 = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-3C-Part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \n",
    "node_datasets = [training1, training2, training3]\n",
    "num_nodes = 3\n",
    "global_updates = 5\n",
    "\n",
    "# Define model training parameters\n",
    "loss_fct = \"categorical_crossentropy\"\n",
    "metrics = ['accuracy']\n",
    "local_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(model, node, x_train, y_train): \n",
    "    filepath = 'C:/Users/UX430/Documents/thesis/code/models/node'+str(node)+'id611.hdf5'\n",
    "    callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "                patience = 10 # Stop after 10 steps with lower accuracy\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath = filepath, # file where the checkpoint is saved\n",
    "                monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "                save_best_only = True)]# Only save model if it is the best\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fct, metrics=metrics)\n",
    "    history = model.fit(x_train, y_train, epochs=local_epochs, validation_split=0.2, callbacks=callbacks, batch_size=2048)\n",
    "    return model, history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(grad_list):\n",
    "    # Calculate mean gradient for each layer\n",
    "    mean_grad = [tf.reduce_mean(layer_grads, axis=0) for layer_grads in zip(*grad_list)]\n",
    "    \n",
    "    return mean_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_model((24,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = preprocessing(training1)\n",
    "x2, y2 = preprocessing(training2)\n",
    "x3, y3 = preprocessing(training3)\n",
    "xbasic, ybasic = preprocessing(test_basic)\n",
    "xplus, yplus = preprocessing(test_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values saved each iteration \n",
    "loss_it = []\n",
    "accuracy_it = []\n",
    "f1_it = []\n",
    "precision_it = []\n",
    "recall_it = []\n",
    "\n",
    "for i in range(global_updates): \n",
    "    gradients_list = []\n",
    "    for node in range(num_nodes): \n",
    "        cp = global_model # create a copy of the global model\n",
    "        if node == 0:\n",
    "            x, y = x1, y1\n",
    "        elif node == 1:\n",
    "            x, y = x2, y2\n",
    "        else:\n",
    "            x, y = x3, y3\n",
    "        local_model, local_loss, local_acc, local_val_loss, local_val_acc = train_local_model(cp, node, x, y)\n",
    "        with tf.GradientTape() as tape: \n",
    "            predictions = local_model(x)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, predictions))\n",
    "        gradients = tape.gradient(loss, local_model.trainable_variables)\n",
    "        gradients_list.append(gradients)\n",
    "\n",
    "    avg_grad = aggregate(gradients_list)\n",
    "    optimizer.apply_gradients(zip(avg_grad, global_model.trainable_variables)) # apply gradients to global model\n",
    "    loss_basic, accuracy_basic, f1_basic, precision_basic, recall_basic = evaluation(global_model, xbasic, ybasic) \n",
    "    loss_plus, accuracy_plus, f1_plus, precision_plus, recall_plus = evaluation(global_model, xplus, yplus)\n",
    "\n",
    "    loss_it.append([loss_basic, loss_plus])\n",
    "    accuracy_it.append([accuracy_basic, accuracy_plus])\n",
    "    f1_it.append([f1_basic, f1_plus])\n",
    "    precision_it.append([precision_basic, precision_plus])\n",
    "    recall_it.append([recall_basic, recall_plus])\n",
    "\n",
    "\n",
    "global_model.save('C:/Users/UX430/Documents/thesis/code/models/id611.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss for iterations: \", loss_it)\n",
    "print(\"Accuracy for iterations: \", accuracy_it)\n",
    "print(\"F1 for iterations: \", f1_it)\n",
    "print(\"Precision for iterations: \", precision_it)\n",
    "print(\"Recall for iterations: \", recall_it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
