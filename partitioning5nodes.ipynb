{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning UNSW-NB15-Train-Basic into 5 nodes \n",
    "\n",
    "The partitions made can be balanced/ unbalanced, with 5 nodes. Attacks might appear in all nodes or only a subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for array\n",
    "import pandas as pd  # for csv files and dataframe\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import seaborn as sns  # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pickle  # To load data int disk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import auc, f1_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UNSW-NB15-Train-Basic dataset \n",
    "complete = pd.read_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(dataset): \n",
    "    # Get only the rows that contain state (PAR, ACC, ECO, CON, FIN, INT, REQ, RST) and proto (igmp, arp, icmp, udp, tcp, ipv6-icmp, rarp)\n",
    "    dataset = dataset[dataset['state'].isin(['PAR', 'ACC', 'ECO', 'CON', 'FIN', 'INT', 'REQ', 'RST'])]\n",
    "    dataset = dataset[dataset['proto'].isin(['igmp', 'arp', 'icmp', 'udp', 'tcp', 'ipv6-icmp', 'rarp'])]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = filter(complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id = 5A : Partition with 5 balanced nodes \n",
    "\n",
    "All of the traffic represented in the 5 nodes. \n",
    "\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part1 (77295): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 32008 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 1783 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part2 (77295): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 32008 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 1783 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part3 (77295): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 32008 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 1782 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part4 (77294): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 32008 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 1782 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part5 (77294): \n",
    "    - Normal: 38648 (%)\n",
    "    - Generic: 32009 (%)\n",
    "    - Exploits: 4273 (%)\n",
    "    - DoS: 582 (%)\n",
    "    - Reconnaissance: 1782 (%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "\n",
    "normal1 = complete[complete['label'] == 0].iloc[:38647]\n",
    "normal2 = complete[complete['label'] == 0].iloc[38647:38647*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[38647*2:38647*3]\n",
    "normal4 = complete[complete['label'] == 0].iloc[38647*3:38647*4]\n",
    "normal5 = complete[complete['label'] == 0].iloc[38647*4:(38647*5+1)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:32008]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[32008:32008*2]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*2:32008*3]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*3:32008*4]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*4:]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:4274]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274:4274*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*2:4274*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*3:4274*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*4:]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:1783]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783:1783*2]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783*2:1783*3-1]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783*3-1:(1783*4-2)]\n",
    "recon5 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(1783*4-2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([normal4, generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([normal5, generic5, exploits5, dos5, recon5])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5A-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5A-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5A-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5A-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5A-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id = 5B : Partition with 5 balanced nodes \n",
    "\n",
    "\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part1 (77294): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 31459 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 2914 (%)\n",
    "    - Reconnaissance: 0 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part2 (77294): \n",
    "    - Normal: 38647 (%)\n",
    "    - Generic: 32145 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 0 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part3 (77295): \n",
    "    - Normal: 70793 (%)\n",
    "    - Generic: 0 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 0 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part4 (77295): \n",
    "    - Normal: 0 (%)\n",
    "    - Generic: 70793 (%)\n",
    "    - Exploits: 4274 (%)\n",
    "    - DoS: 0 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part5 (77295): \n",
    "    - Normal: 45149 (%)\n",
    "    - Generic: 23211 (%)\n",
    "    - Exploits: 4273 (%)\n",
    "    - DoS: 0 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "\n",
    "normal1 = complete[complete['label'] == 0].iloc[:38647]\n",
    "normal2 = complete[complete['label'] == 0].iloc[38647:38647*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[38647*2:(38647*2+70793)]\n",
    "normal4 = complete[complete['label'] == 0].iloc[(38647*2+70793):(38647*2+70793+45149)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:31459]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[31459:(31459+32145)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(31459+32145):(31459+32145+70793)]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[(31459+32145+70793):]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:4274]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*1:4274*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*2:4274*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*3:4274*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*4:]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:2228]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[2228:(2228*2)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*2):(2228*3)]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, recon1])\n",
    "part3 = pd.concat([normal3, exploits3, recon2])\n",
    "part4 = pd.concat([generic3, exploits4, recon3])\n",
    "part5 = pd.concat([normal4, generic4, exploits5, recon4])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5B-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5B-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5B-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5B-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5B-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id = 5C : Partition with 5 unbalanced nodes \n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part1 (): \n",
    "    - Normal: 24154 (%)\n",
    "    - Generic: 7865 (%)\n",
    "    - Exploits: 5342 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part2 (): \n",
    "    - Normal: 24154 (%)\n",
    "    - Generic: 7865 (%)\n",
    "    - Exploits: 0 (%)\n",
    "    - DoS: 583  (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part3 (): \n",
    "    - Normal: 48308 (%)\n",
    "    - Generic: 7865 (%)\n",
    "    - Exploits: 5342 (%)\n",
    "    - DoS: 583  (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part4 (): \n",
    "    - Normal: 0 (%)\n",
    "    - Generic: 60000 (%)\n",
    "    - Exploits: 5342 (%)\n",
    "    - DoS: 583 (%)\n",
    "    - Reconnaissance: 2228 (%)\n",
    "\n",
    "- UNSW-NB15-Train-Basic-Part5 (): \n",
    "    - Normal: 96620 (%)\n",
    "    - Generic: 76446 (%)\n",
    "    - Exploits: 5343 (%)\n",
    "    - DoS: 582 (%)\n",
    "    - Reconnaissance: 0 (%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "\n",
    "normal1 = complete[complete['label'] == 0].iloc[:24154]\n",
    "normal2 = complete[complete['label'] == 0].iloc[24154:24154*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[24154*2:(24154*2+48308)]\n",
    "normal4 = complete[complete['label'] == 0].iloc[(24154*2+48308):(24154*2+48308+96620)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:7865]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[7865:(7865*2)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*2):(7865*3)]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3):(7865*3+60000)]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3+60000):]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:5342]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[5342:(5342*2)]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[((5342*2)):(5342*3)]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[(5342*3):]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:2228]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[2228:(2228*2)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*2):(2228*3)]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits2, dos3, recon3])\n",
    "part4 = pd.concat([generic4, exploits3, dos4, recon4])\n",
    "part5 = pd.concat([normal4, generic5, exploits4, dos5])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5C-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Filt5C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification datasets \n",
    "\n",
    "Normal traffic is not considered, as it's going to feed models that categorize attacks (when detection model detects attack). Information about percentages and distribution of attacks is included in the \"datasets.xslx\" file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAT5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:32008]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[32008:32008*2]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*2:32008*3]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*3:32008*4]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*4:]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:4274]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274:4274*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*2:4274*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*3:4274*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*4:]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:1782]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1782:1782*2]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1782*2:1782*3]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1782*3:(1782*4+1)]\n",
    "recon5 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(1782*4+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([generic5, exploits5, dos5, recon5])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5A-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5A-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5A-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5A-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5A-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAT5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:600]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[600:600*2]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[600*2:600*3]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[600*3:600*4]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[600*4:600*5]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:600]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[600:600*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[600*2:600*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[600*3:600*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[600*4:600*5]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:600]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[600:600*2]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[600*2:600*3]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[600*3:600*4]\n",
    "recon5 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[600*4:600*5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([generic5, exploits5, dos5, recon5])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5B-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5B-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5B-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5B-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5B-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAT5C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:1457]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[1457:(1457+3728)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(1457+3728):(1457+3728+17642)]\n",
    "\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:3727]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[3727:(17642+3727)]\n",
    "\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:1457]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[1457:]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:1457]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(1457):(1457+3727)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(1457+3727):(1457+3727+3728)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([generic1, dos1])\n",
    "part2 = pd.concat([dos2, recon1])\n",
    "part3 = pd.concat([exploits1, recon2])\n",
    "part4 = pd.concat([generic2,recon3])\n",
    "part5 = pd.concat([generic3, exploits2])\n",
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5C-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Cat5C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted datasets\n",
    "Corruption of Filt5B node 1 and Filt5C node 1 and node 3 -> percentages of 5, 10 and 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corr5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "\n",
    "normal1 = complete[complete['label'] == 0].iloc[:38647]\n",
    "normal2 = complete[complete['label'] == 0].iloc[38647:38647*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[38647*2:38647*3]\n",
    "normal4 = complete[complete['label'] == 0].iloc[38647*3:38647*4]\n",
    "normal5 = complete[complete['label'] == 0].iloc[38647*4:(38647*5+1)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:32008]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[32008:32008*2]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*2:32008*3]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*3:32008*4]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[32008*4:]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:4274]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274:4274*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*2:4274*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*3:4274*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*4:]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:1783]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783:1783*2]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783*2:1783*3-1]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[1783*3-1:(1783*4-2)]\n",
    "recon5 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(1783*4-2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([normal4, generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([normal5, generic5, exploits5, dos5, recon5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15A-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15A-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15A-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15A-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15A-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([normal4, generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([normal5, generic5, exploits5, dos5, recon5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15A-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15A-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15A-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15A-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15A-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNSW-NB15-Train-Basic-PartN, N = 1,2,3,4,5 dataset and export to csv\n",
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits3, dos3, recon3])\n",
    "part4 = pd.concat([normal4, generic4, exploits4, dos4, recon4])\n",
    "part5 = pd.concat([normal5, generic5, exploits5, dos5, recon5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.0125, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.0125, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15A-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15A-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15A-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15A-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15A-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corr5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "\n",
    "normal1 = complete[complete['label'] == 0].iloc[:38647]\n",
    "normal2 = complete[complete['label'] == 0].iloc[38647:38647*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[38647*2:(38647*2+70793)]\n",
    "normal4 = complete[complete['label'] == 0].iloc[(38647*2+70793):(38647*2+70793+45149)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:31459]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[31459:(31459+32145)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(31459+32145):(31459+32145+70793)]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[(31459+32145+70793):]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:4274]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*1:4274*2]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*2:4274*3]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*3:4274*4]\n",
    "exploits5 = complete[complete['attack_cat'] == \"exploits\"].iloc[4274*4:]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:2228]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[2228:(2228*2)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*2):(2228*3)]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = pd.concat([normal1, generic1, exploits1, dos1])\n",
    "part2 = pd.concat([normal2, generic2, exploits2, recon1])\n",
    "part3 = pd.concat([normal3, exploits3, recon2])\n",
    "part4 = pd.concat([generic3, exploits4, recon3])\n",
    "part5 = pd.concat([normal4, generic4, exploits5, recon4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15B-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15B-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15B-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15B-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15B-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15B-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15B-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15B-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15B-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15B-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.125, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.125, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15B-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15B-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15B-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15B-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15B-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corr5C node 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "normal1 = complete[complete['label'] == 0].iloc[:24154]\n",
    "normal2 = complete[complete['label'] == 0].iloc[24154:24154*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[24154*2:(24154*2+48308)]\n",
    "normal4 = complete[complete['label'] == 0].iloc[(24154*2+48308):(24154*2+48308+96620)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:7865]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[7865:(7865*2)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*2):(7865*3)]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3):(7865*3+60000)]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3+60000):]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:5342]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[5342:(5342*2)]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[((5342*2)):(5342*3)]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[(5342*3):]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:2228]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[2228:(2228*2)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*2):(2228*3)]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits2, dos3, recon3])\n",
    "part4 = pd.concat([generic4, exploits3, dos4, recon4])\n",
    "part5 = pd.concat([normal4, generic5, exploits4, dos5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.025, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15C-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%15C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.05, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15C-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%15C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part1_sample_label0 = part1[part1['label'] == 0].sample(frac=0.125, random_state=42)\n",
    "part1_sample_label0['label'] = 1\n",
    "part1_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part1_sample_label1 = part1[part1['label'] == 1].sample(frac=0.125, random_state=42)\n",
    "part1_sample_label1['label'] = 0\n",
    "part1_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part1_concatenated = pd.concat([part1.drop(part1_sample_label0.index).drop(part1_sample_label1.index), part1_sample_label0, part1_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part1_changed = part1_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15C-Part2.csv', index=False)\n",
    "part3.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%15C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corr 5C node 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitions \n",
    "normal1 = complete[complete['label'] == 0].iloc[:24154]\n",
    "normal2 = complete[complete['label'] == 0].iloc[24154:24154*2]\n",
    "normal3 = complete[complete['label'] == 0].iloc[24154*2:(24154*2+48308)]\n",
    "normal4 = complete[complete['label'] == 0].iloc[(24154*2+48308):(24154*2+48308+96620)]\n",
    "\n",
    "generic1 = complete[complete['attack_cat'] == \"generic\"].iloc[:7865]\n",
    "generic2 = complete[complete['attack_cat'] == \"generic\"].iloc[7865:(7865*2)]\n",
    "generic3 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*2):(7865*3)]\n",
    "generic4 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3):(7865*3+60000)]\n",
    "generic5 = complete[complete['attack_cat'] == \"generic\"].iloc[(7865*3+60000):]\n",
    "\n",
    "exploits1 = complete[complete['attack_cat'] == \"exploits\"].iloc[:5342]\n",
    "exploits2 = complete[complete['attack_cat'] == \"exploits\"].iloc[5342:(5342*2)]\n",
    "exploits3 = complete[complete['attack_cat'] == \"exploits\"].iloc[((5342*2)):(5342*3)]\n",
    "exploits4 = complete[complete['attack_cat'] == \"exploits\"].iloc[(5342*3):]\n",
    "\n",
    "dos1 = complete[complete['attack_cat'] == \"dos\"].iloc[:583]\n",
    "dos2 = complete[complete['attack_cat'] == \"dos\"].iloc[583:(583*2)]\n",
    "dos3 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*2):(583*3)]\n",
    "dos4 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*3):(583*4)]\n",
    "dos5 = complete[complete['attack_cat'] == \"dos\"].iloc[(583*4):]\n",
    "\n",
    "recon1 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[:2228]\n",
    "recon2 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[2228:(2228*2)]\n",
    "recon3 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*2):(2228*3)]\n",
    "recon4 = complete[complete['attack_cat'] == \"reconnaissance\"].iloc[(2228*3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = pd.concat([normal1, generic1, exploits1, dos1, recon1])\n",
    "part2 = pd.concat([normal2, generic2, dos2, recon2])\n",
    "part3 = pd.concat([normal3, generic3, exploits2, dos3, recon3])\n",
    "part4 = pd.concat([generic4, exploits3, dos4, recon4])\n",
    "part5 = pd.concat([normal4, generic5, exploits4, dos5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part3_sample_label0 = part3[part3['label'] == 0].sample(frac=0.025, random_state=42)\n",
    "part3_sample_label0['label'] = 1\n",
    "part3_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part3_sample_label1 = part3[part3['label'] == 1].sample(frac=0.025, random_state=42)\n",
    "part3_sample_label1['label'] = 0\n",
    "part3_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part3_concatenated = pd.concat([part3.drop(part3_sample_label0.index).drop(part3_sample_label1.index), part3_sample_label0, part3_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part3_changed = part3_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%35C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%35C-Part2.csv', index=False)\n",
    "part3_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%35C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%35C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr5%35C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part3_sample_label0 = part3[part3['label'] == 0].sample(frac=0.05, random_state=42)\n",
    "part3_sample_label0['label'] = 1\n",
    "part3_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part3_sample_label1 = part3[part3['label'] == 1].sample(frac=0.05, random_state=42)\n",
    "part3_sample_label1['label'] = 0\n",
    "part3_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part3_concatenated = pd.concat([part3.drop(part3_sample_label0.index).drop(part3_sample_label1.index), part3_sample_label0, part3_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part3_changed = part3_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%35C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%35C-Part2.csv', index=False)\n",
    "part3_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%35C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%35C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr10%35C-Part5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2.5% of samples with label 0 and change their label to 1 and attack_cat to \"generic\"\n",
    "part3_sample_label0 = part3[part3['label'] == 0].sample(frac=0.125, random_state=42)\n",
    "part3_sample_label0['label'] = 1\n",
    "part3_sample_label0['attack_cat'] = \"generic\"\n",
    "\n",
    "# Randomly select 2.5% of samples with label 1 and change their label to 0 and attack_cat to \"normal\"\n",
    "part3_sample_label1 = part3[part3['label'] == 1].sample(frac=0.125, random_state=42)\n",
    "part3_sample_label1['label'] = 0\n",
    "part3_sample_label1['attack_cat'] = \"normal\"\n",
    "\n",
    "# Concatenate the modified parts and the original part1, dropping the modified samples\n",
    "part3_concatenated = pd.concat([part3.drop(part3_sample_label0.index).drop(part3_sample_label1.index), part3_sample_label0, part3_sample_label1])\n",
    "\n",
    "# Shuffle the rows of the concatenated dataframe\n",
    "part3_changed = part3_concatenated.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%35C-Part1.csv', index=False)\n",
    "part2.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%35C-Part2.csv', index=False)\n",
    "part3_changed.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%35C-Part3.csv', index=False)\n",
    "part4.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%35C-Part4.csv', index=False)\n",
    "part5.to_csv('C:/Users/UX430/Documents/thesis/datasets/UNSW-NB15/UNSW-NB15-Train-Basic-Corr25%35C-Part5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
